{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì „ì²˜ë¦¬ ë° í† í°í™” ê³¼ì • step by step\n",
    "\n",
    "\n",
    "### Soynlp (â€˜ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸â€™ë„ì„œ ì°¸ê³ )\n",
    "\n",
    "Non-nullë§Œ ì¶”ì¶œ -> ê´‘ê³ ì„± ê¸€ ì œê±° -> soynlp normalizer -> soyspacing -> ì •ê·œì‹ ì ìš©(í•œê¸€ ë° ì˜ì–´ ë¬¸ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±° + urlí˜•íƒœ ë¬¸ìì—´ ì œê±°) -> LRNounExtractor_v2 ëª…ì‚¬ì¶”ì¶œ -> ë¶ˆìš©ì–´ ì œì™¸\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZ5wursIwIFd"
   },
   "source": [
    "## 0. í™˜ê²½ setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ê´‘ê³  ì œê±°ê¹Œì§€ ì •ì œëœ ë°ì´í„° ë¡œë“œ\n",
    "\n",
    "Non-nullë§Œ ì¶”ì¶œ -> ê´‘ê³ ì„± ê¸€ ì œê±° ê¹Œì§€ ì²˜ë¦¬ë˜ì–´ csvì €ì¥ëœ data ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Like</th>\n",
       "      <th>Content</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>SNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://blog.naver.com/lsy_sweet/222457159636</td>\n",
       "      <td>2021. 8. 15. 9:58</td>\n",
       "      <td>12.0</td>\n",
       "      <td>ì•ˆë…•í•˜ì„¸ìš”, ì¹œí™˜ê²½ ì‚´ë¦¼ì„ í•˜ë©° í™˜ê²½ ë³´ì¡´ì„ ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ë‹¨ìˆœì´ ì…ë‹ˆë‹¤. ê·¸ë™ì•ˆ ...</td>\n",
       "      <td>#ê³ ì²´ì¹˜ì•½ #ì œë¡œì›¨ì´ìŠ¤íŠ¸ #ê³ ì²´ì¹˜ì•½ë³¸í‹° #ë³¸í‹°ì¹˜ì•½ #Vontee</td>\n",
       "      <td>blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://blog.naver.com/newblack76/222468647865</td>\n",
       "      <td>2021. 8. 15. 9:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ëª¨ë‘ë“¤ ì•ˆë…•í•˜ì…¨ì–´ìš”~ ì…ì¶”ê°€ ì§€ë‚˜ê³  ë§ë³µì´ ì§€ë‚˜ë‹ˆ ì´ì   ë°”ëŒì´ í‹€ë ¤ì§„ë“¯í•´ìš”^^ ê¸ˆë°©...</td>\n",
       "      <td>#êµ­ì‚°ì²œì—°ìˆ˜ì„¸ë¯¸ #ì œë¡œì›¨ì´ìŠ¤íŠ¸ì„ ë¬¼ #êµ­ë‚´ì‚°ì²œì—°ìˆ˜ì„¸ë¯¸ #ë¶ì–´í‘œìˆ˜ì„¸ë¯¸ #ì˜ë¼ì“°ëŠ”ìˆ˜ì„¸ë¯¸ ...</td>\n",
       "      <td>blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://blog.naver.com/somcandy117/222460526981</td>\n",
       "      <td>2021. 8. 15. 9:10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ì˜¤ë¹ ê°€ ìœ ë¦¬ë¹¨ëŒ€ì˜ ë”±ë”±í•œ ì§ˆê°ì´ ì‹«ë‹¤ê³  í•´ì„œ 100í¼ ìì—°ì£¼ì˜ í’€ë¹¨ëŒ€ë¥¼ ì‚¬ë´¤ë‹¤ ë°°ì†¡...</td>\n",
       "      <td>#í’€ë¹¨ëŒ€ #ìì—°ë¹¨ëŒ€ #ì œë¡œì›¨ì´ìŠ¤íŠ¸ #ìƒë¶„í•´ë¹¨ëŒ€ #ë² íŠ¸ë‚¨í’€ë¹¨ëŒ€ #ì¹œí™˜ê²½ë¹¨ëŒ€ #í™˜ê²½ë³´í˜¸...</td>\n",
       "      <td>blog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               URL                Date  Like  \\\n",
       "0    https://blog.naver.com/lsy_sweet/222457159636   2021. 8. 15. 9:58  12.0   \n",
       "1   https://blog.naver.com/newblack76/222468647865   2021. 8. 15. 9:49   NaN   \n",
       "2  https://blog.naver.com/somcandy117/222460526981   2021. 8. 15. 9:10   2.0   \n",
       "\n",
       "                                             Content  \\\n",
       "0  ì•ˆë…•í•˜ì„¸ìš”, ì¹œí™˜ê²½ ì‚´ë¦¼ì„ í•˜ë©° í™˜ê²½ ë³´ì¡´ì„ ìœ„í•´ ë…¸ë ¥í•˜ëŠ” ë‹¨ìˆœì´ ì…ë‹ˆë‹¤. ê·¸ë™ì•ˆ ...   \n",
       "1  ëª¨ë‘ë“¤ ì•ˆë…•í•˜ì…¨ì–´ìš”~ ì…ì¶”ê°€ ì§€ë‚˜ê³  ë§ë³µì´ ì§€ë‚˜ë‹ˆ ì´ì   ë°”ëŒì´ í‹€ë ¤ì§„ë“¯í•´ìš”^^ ê¸ˆë°©...   \n",
       "2  ì˜¤ë¹ ê°€ ìœ ë¦¬ë¹¨ëŒ€ì˜ ë”±ë”±í•œ ì§ˆê°ì´ ì‹«ë‹¤ê³  í•´ì„œ 100í¼ ìì—°ì£¼ì˜ í’€ë¹¨ëŒ€ë¥¼ ì‚¬ë´¤ë‹¤ ë°°ì†¡...   \n",
       "\n",
       "                                             Hashtag   SNS  \n",
       "0                #ê³ ì²´ì¹˜ì•½ #ì œë¡œì›¨ì´ìŠ¤íŠ¸ #ê³ ì²´ì¹˜ì•½ë³¸í‹° #ë³¸í‹°ì¹˜ì•½ #Vontee  blog  \n",
       "1  #êµ­ì‚°ì²œì—°ìˆ˜ì„¸ë¯¸ #ì œë¡œì›¨ì´ìŠ¤íŠ¸ì„ ë¬¼ #êµ­ë‚´ì‚°ì²œì—°ìˆ˜ì„¸ë¯¸ #ë¶ì–´í‘œìˆ˜ì„¸ë¯¸ #ì˜ë¼ì“°ëŠ”ìˆ˜ì„¸ë¯¸ ...  blog  \n",
       "2  #í’€ë¹¨ëŒ€ #ìì—°ë¹¨ëŒ€ #ì œë¡œì›¨ì´ìŠ¤íŠ¸ #ìƒë¶„í•´ë¹¨ëŒ€ #ë² íŠ¸ë‚¨í’€ë¹¨ëŒ€ #ì¹œí™˜ê²½ë¹¨ëŒ€ #í™˜ê²½ë³´í˜¸...  blog  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blog = pd.read_csv('./zerowaste/processed4200/blog_non-null_no-ads_4200.csv')\n",
    "df_blog.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Like</th>\n",
       "      <th>Content</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>SNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vege_yony</td>\n",
       "      <td>2021-08-15 23:59:36+00:00</td>\n",
       "      <td>49</td>\n",
       "      <td>- ê·¸ë¦­ìš”ê±°íŠ¸ ë‹¤ë“¤ ì‹ì‚¬ë¡œ ë¨¹ì§€ë§Œ ê°„ì‹ìœ¼ë¡œ ë¨¹ëŠ” ì‚¬ëŒ ë‚˜ì•¼ ë‚˜- ì¹´í˜ ì¡°ê° ë¹„ê±´ê·¸ë¦­...</td>\n",
       "      <td>['#ë¹„ê±´ê·¸ë¦­ìš”ê±°íŠ¸', '#ì‘¥ë¸Œë¼ìš°ë‹ˆ', '#ë¹„ê±´ì‘¥ë””ì €íŠ¸']</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eco_kapegg</td>\n",
       "      <td>2021-08-15 23:45:31+00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>â € ë”ìš´ë‚ ğŸ¥² ìŒë£Œí•„ìˆ˜ì¥ì°©í•´ì•¼ë˜ì¥¬ ì¤‘ë¶€ì‚¬ë¶€ì†Œ ì¡°ê³¼ì¥ë‹˜ì˜ í…€ë¸”ëŸ¬ ìš©ê¸°ë‚´ğŸ‘ğŸ‘ â € í…€ë¸”ëŸ¬ì—...</td>\n",
       "      <td>['#ìš©ê¸°ë‚´', '#ì œë¡œì›¨ì´ìŠ¤íŠ¸', '#í™˜ê²½ë³´í˜¸', '#í…€ë¸”ëŸ¬', '#ìº í˜ì¸', '...</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eco_kapegg</td>\n",
       "      <td>2021-08-15 23:42:02+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>â € ì¤‘ë¶€ì‚¬ë¬´ì†Œ ì„œëŒ€ë¦¬ë‹˜ì˜ ì£¼ë§ ì¥ë³´ê¸°ğŸ¤— ë¯¸ë¦¬ ì¥ë°”êµ¬ë‹ˆë¥¼ ì±™ê²¨ì„œ ë´‰íˆ¬ì‚¬ìš©ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤...</td>\n",
       "      <td>['#ì œë¡œì›¨ì´ìŠ¤íŠ¸', '#ìº í˜ì¸', '#ì¥ë°”êµ¬ë‹ˆ', '#ì¼íšŒìš©í’ˆì¤„ì´ê¸°', '#ë´‰íˆ¬ì¤„...</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          URL                       Date Like  \\\n",
       "0   vege_yony  2021-08-15 23:59:36+00:00   49   \n",
       "1  eco_kapegg  2021-08-15 23:45:31+00:00   16   \n",
       "2  eco_kapegg  2021-08-15 23:42:02+00:00   14   \n",
       "\n",
       "                                             Content  \\\n",
       "0  - ê·¸ë¦­ìš”ê±°íŠ¸ ë‹¤ë“¤ ì‹ì‚¬ë¡œ ë¨¹ì§€ë§Œ ê°„ì‹ìœ¼ë¡œ ë¨¹ëŠ” ì‚¬ëŒ ë‚˜ì•¼ ë‚˜- ì¹´í˜ ì¡°ê° ë¹„ê±´ê·¸ë¦­...   \n",
       "1  â € ë”ìš´ë‚ ğŸ¥² ìŒë£Œí•„ìˆ˜ì¥ì°©í•´ì•¼ë˜ì¥¬ ì¤‘ë¶€ì‚¬ë¶€ì†Œ ì¡°ê³¼ì¥ë‹˜ì˜ í…€ë¸”ëŸ¬ ìš©ê¸°ë‚´ğŸ‘ğŸ‘ â € í…€ë¸”ëŸ¬ì—...   \n",
       "2  â € ì¤‘ë¶€ì‚¬ë¬´ì†Œ ì„œëŒ€ë¦¬ë‹˜ì˜ ì£¼ë§ ì¥ë³´ê¸°ğŸ¤— ë¯¸ë¦¬ ì¥ë°”êµ¬ë‹ˆë¥¼ ì±™ê²¨ì„œ ë´‰íˆ¬ì‚¬ìš©ì„ ì¤„ì˜€ìŠµë‹ˆë‹¤...   \n",
       "\n",
       "                                             Hashtag        SNS  \n",
       "0                  ['#ë¹„ê±´ê·¸ë¦­ìš”ê±°íŠ¸', '#ì‘¥ë¸Œë¼ìš°ë‹ˆ', '#ë¹„ê±´ì‘¥ë””ì €íŠ¸']  instagram  \n",
       "1  ['#ìš©ê¸°ë‚´', '#ì œë¡œì›¨ì´ìŠ¤íŠ¸', '#í™˜ê²½ë³´í˜¸', '#í…€ë¸”ëŸ¬', '#ìº í˜ì¸', '...  instagram  \n",
       "2  ['#ì œë¡œì›¨ì´ìŠ¤íŠ¸', '#ìº í˜ì¸', '#ì¥ë°”êµ¬ë‹ˆ', '#ì¼íšŒìš©í’ˆì¤„ì´ê¸°', '#ë´‰íˆ¬ì¤„...  instagram  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_insta = pd.read_csv('./zerowaste/processed4200/insta_non-null_no-ads_4200.csv')\n",
    "df_insta.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Like</th>\n",
       "      <th>Content</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>SNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cafe.naver.com/applestore99/1704?art=a...</td>\n",
       "      <td>2021.08.15. 23:25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>â€‹â€‹â€‹ì¨ë³´ê³  í‰ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ê°“ì„±ë¹„ ìƒ´í‘¸ë°” ì´êµ¬ìš”. ê°€ê²©í›„ê¸°ë³„ ìˆœìœ„ëª¨ìŒâ€‹â€‹â€‹  &lt;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cafe.naver.com/myomahealing/151141?art...</td>\n",
       "      <td>2021.08.15. 23:01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>ê¸€ì“°ê¸° ì „ ê¸°ë³¸ ì •ë³´ê°€ ìˆëŠ” 'ê³µë¶€í•©ì‹œë‹¤' ê²Œì‹œíŒì„ í™•ì¸í•˜ì‹œê³ , ì¤‘ë³µë˜ëŠ” ë¬¸ì˜ê¸€ì´ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://cafe.naver.com/donggubat0/72?art=aW50Z...</td>\n",
       "      <td>2021.08.15. 22:39</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ë™êµ¬ë°­ê³¼ ì²« ì¸ì—°ì„ ë§ºê²Œ í•´ì¤€, ê·¸ ì²« ì¸ì—°ì´í›„ ì­‰ ì €ì˜ ìµœì• í…œì€ 'ì˜¬ë°”ë¥¸ ì„¤ê±°ì§€ ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL               Date  Like  \\\n",
       "0  https://cafe.naver.com/applestore99/1704?art=a...  2021.08.15. 23:25   0.0   \n",
       "1  https://cafe.naver.com/myomahealing/151141?art...  2021.08.15. 23:01   3.0   \n",
       "2  https://cafe.naver.com/donggubat0/72?art=aW50Z...  2021.08.15. 22:39   2.0   \n",
       "\n",
       "                                             Content Hashtag   SNS  \n",
       "0  â€‹â€‹â€‹ì¨ë³´ê³  í‰ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ê°“ì„±ë¹„ ìƒ´í‘¸ë°” ì´êµ¬ìš”. ê°€ê²©í›„ê¸°ë³„ ìˆœìœ„ëª¨ìŒâ€‹â€‹â€‹  <...     NaN  cafe  \n",
       "1  ê¸€ì“°ê¸° ì „ ê¸°ë³¸ ì •ë³´ê°€ ìˆëŠ” 'ê³µë¶€í•©ì‹œë‹¤' ê²Œì‹œíŒì„ í™•ì¸í•˜ì‹œê³ , ì¤‘ë³µë˜ëŠ” ë¬¸ì˜ê¸€ì´ ...     NaN  cafe  \n",
       "2  ë™êµ¬ë°­ê³¼ ì²« ì¸ì—°ì„ ë§ºê²Œ í•´ì¤€, ê·¸ ì²« ì¸ì—°ì´í›„ ì­‰ ì €ì˜ ìµœì• í…œì€ 'ì˜¬ë°”ë¥¸ ì„¤ê±°ì§€ ...     NaN  cafe  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cafe = pd.read_csv('./zerowaste/processed4200/cafe_non-null_no-ads_4200.csv')\n",
    "df_cafe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200\n",
      "4200\n",
      "4200\n"
     ]
    }
   ],
   "source": [
    "print(len(df_blog))\n",
    "print(len(df_insta))\n",
    "print(len(df_cafe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2rSCVquxIFZ"
   },
   "source": [
    "## 2. Soyspacing & SOYNLP normalizer ì²˜ë¦¬\n",
    "\n",
    "#### soyspacing\n",
    "\n",
    "ë„ì–´ì“°ê¸° ì—ëŸ¬ ì²˜ë¦¬ ë° ì´ëª¨í‹°ì½˜, ë°˜ë³µ ê¸€ì ì •ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soyspacing\n",
    "print(soyspacing.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soyspacing.countbase import RuleDict, CountSpace\n",
    "\n",
    "model2 = CountSpace()\n",
    "model2.load_model('./soyspacing/demo_model/test.model', json_format=False)\n",
    "\n",
    "verbose=False\n",
    "mc = 10  # min_count\n",
    "ft = 0.3 # force_abs_threshold\n",
    "nt =-0.3 # nonspace_threshold\n",
    "st = 0.3 # space_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spacing(sent):\n",
    "    sent_corrected, tags = model2.correct(\n",
    "        sent,\n",
    "        verbose=verbose,\n",
    "        force_abs_threshold=ft,\n",
    "        nonspace_threshold=nt,\n",
    "        space_threshold=st,\n",
    "        min_count=mc\n",
    "    )\n",
    "    return sent_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blog['Content'] = df_blog['Content'].apply(lambda x: fix_spacing(x))\n",
    "df_insta['Content'] = df_insta['Content'].apply(lambda x: fix_spacing(x))\n",
    "df_cafe['Content'] = df_cafe['Content'].apply(lambda x: fix_spacing(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqkP1CPH1XJl"
   },
   "source": [
    "#### SOYNLPì˜ Normalizer\n",
    "\n",
    "ëŒ€í™” ë°ì´í„°, ëŒ“ê¸€ ë°ì´í„°ì— ë“±ì¥í•˜ëŠ” ë°˜ë³µë˜ëŠ” ì´ëª¨í‹°ì½˜ì˜ ì •ë¦¬ ë° í•œê¸€, í˜¹ì€ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸°ê¸° ìœ„í•œ í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆì‹œ)\n",
    "\n",
    "emoticon_normalize('ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ì¿ ã…œã…œã…œã…œã…œã…œ', num_repeats=3)\n",
    "\n",
    "ê²°ê³¼:  'ã…‹ã…‹ã…‹ã…œã…œã…œ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XINPHyxp1Zp-"
   },
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akal6JXg1whG"
   },
   "outputs": [],
   "source": [
    "def normalize_all(df):\n",
    "    for idx in df.index:\n",
    "        df.loc[idx,'Content'] = emoticon_normalize(df.loc[idx,'Content'], num_repeats=3) \n",
    "        df.loc[idx,'Content'] = repeat_normalize(df.loc[idx,'Content'], num_repeats=2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mp2iX9Ry1Zl6",
    "outputId": "ce6b11e8-84cf-41af-8a38-3e75a90572fa"
   },
   "outputs": [],
   "source": [
    "df_blog_norm = normalize_all(df_blog)\n",
    "df_insta_norm = normalize_all(df_insta)\n",
    "df_cafe_norm = normalize_all(df_cafe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ê²Œì‹œê¸€ë³„ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸í™” + ì •ê·œì‹ ì ìš©\n",
    "\n",
    "ê²Œì‹œê¸€ ë‚´ìš©ì„ ë¬¸ì¥ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë¦¬ìŠ¤íŠ¸í™” í•¨\n",
    "\n",
    "URL ì œì™¸, í•œê¸€ê³¼ ì˜ì–´ë§Œ ë‚¨ë„ë¡ ì •ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentences(text, punc):\n",
    "    # syonlp noun extractor ì‚¬ìš©ì„ ìœ„í•´ ë¬¸ì¥ì‚¬ì´ doublespace ì‚½ì…\n",
    "    sentences=[]; onesentence=\"\"\n",
    "    for ele in text:\n",
    "        onesentence += ele\n",
    "        if ele in punc:\n",
    "            sentences.append(onesentence+\"  \")\n",
    "            onesentence=\"\"\n",
    "    return sentences\n",
    "            \n",
    "def regular_expression(text): \n",
    "    # urlì œê±°\n",
    "    url = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    pattern_url = re.compile(url)\n",
    "    tmp = pattern_url.sub('', text)\n",
    "    # regexì¶”ì¶œ ê·œì¹™: ì˜ì–´ ë˜ëŠ” ë„ì–´ ì“°ê¸°(1 ê°œ)ë¥¼ í¬í•¨í•œ í•œê¸€\n",
    "    pattern = re.compile('[^a-zA-Z| ã„±-ã…£ ê°€-í£]')  \n",
    "    result = pattern.sub('', tmp)  # ìœ„ì— ì„¤ì •í•œ \"hangul\"ê·œì¹™ì„ \"text\"ì— ì ìš©(.sub)ì‹œí‚´\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sep_regex(text,punct):\n",
    "    sentences = separate_sentences(text,punct)\n",
    "    reg_sentences=[]\n",
    "    for s in sentences:\n",
    "        reg_sentences.append(regular_expression(s))\n",
    "    return reg_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEXSUu7HpOSi",
    "outputId": "ec09cfb7-dac0-4c1c-e30a-d93ac6d7abc7"
   },
   "outputs": [],
   "source": [
    "#punctuation = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "punctuation = '''!.?~'''\n",
    "\n",
    "df_blog_norm['Content'] = df_blog_norm['Content'].apply(lambda x: apply_sep_regex(x, punctuation))\n",
    "df_insta_norm['Content'] = df_insta_norm['Content'].apply(lambda x: apply_sep_regex(x, punctuation))\n",
    "df_cafe_norm['Content'] = df_cafe_norm['Content'].apply(lambda x: apply_sep_regex(x, punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elminate_empty_lines(text):\n",
    "    non_empty=[]\n",
    "    for line in text:\n",
    "        if len(line)>0 and line!=' ': non_empty.append(line)\n",
    "    return non_empty         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blog_norm['Content'] = df_blog_norm['Content'].apply(lambda x: elminate_empty_lines(x))\n",
    "df_insta_norm['Content'] = df_insta_norm['Content'].apply(lambda x: elminate_empty_lines(x))\n",
    "df_cafe_norm['Content'] = df_cafe_norm['Content'].apply(lambda x: elminate_empty_lines(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG762IAH7v4H"
   },
   "source": [
    "## 4. í† í°í™” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zrs1KPLOGBqQ"
   },
   "source": [
    "### SOYNLPì˜ Noun Extractor(v.2) í™œìš© \n",
    "\n",
    "soynlpëŠ” í’ˆì‚¬ íƒœê¹…, ë‹¨ì–´ í† í°í™”ë“¤ì„ ì œê³µí•¨.\n",
    "ë¹„ì§€ë„ í•™ìŠµìœ¼ë¡œ ë‹¨ì–´ í† í°í™” ì§„í–‰ - ë°ì´í„°ì—ì„œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ (ì‹ ì¡°ì–´ í¬í•¨) ë‹¨ì–´ë¡œ ë¶„ì„í•¨.\n",
    "\n",
    "soynlp=0.0.46+ ì—ì„œëŠ” ëª…ì‚¬ ì¶”ì¶œê¸° version 2 ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ì „ ë²„ì „ì˜ ëª…ì‚¬ ì¶”ì¶œì˜ ì •í™•ì„±ê³¼ í•©ì„±ëª…ì‚¬ ì¸ì‹ ëŠ¥ë ¥, ì¶œë ¥ë˜ëŠ” ì •ë³´ì˜ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•œ ë²„ì „ì…ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ê° ê²Œì‹œê¸€ë³„ ëª…ì‚¬(+ê²Œì‹œê¸€ë³„ ë¹ˆë„ìˆ˜) ì¶”ì¶œ\n",
    "\n",
    "df['Content']ì˜ ê° ê²Œì‹œê¸€ì„ ë¬¸ì¥ë‹¨ìœ„ë¡œ ë‚˜ë‰œ elementsë¥¼ ê°€ì§„ listì´ë‹¤.\n",
    "\n",
    "sents = df_blog.loc[idx,'Content']\n",
    "\n",
    "sentsë¥¼ noun_extractorì— ë„£ì–´ì„œ ê° ê²Œì‹œê¸€ë³„ ëª…ì‚¬ì™€ ë¹ˆë„ìˆ˜ ë° scoreë¥¼ ì¶”ì¶œí•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(df,freq):\n",
    "    nouns=[]\n",
    "    for idx in df.index:\n",
    "        sents = df.loc[idx,'Content']\n",
    "        try:\n",
    "            nouns_per_posting = noun_extractor.train_extract(\n",
    "            sents,\n",
    "            min_noun_frequency=freq)\n",
    "            nouns.append(nouns_per_posting)\n",
    "        except:\n",
    "            pass\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minfreq=3 ì„¤ì • ì´ìœ  - ì„œë¡ ,ë³¸ë¡ ,ê²°ë¡ ì— ì ì–´ë„ í•œë²ˆì”© ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë¼ë©´ 3+ ì˜ˆìƒí•¨.\n",
    "nouns_minfreq_blog = extract_nouns(df_blog_norm,3)\n",
    "nouns_minfreq_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_minfreq_insta = extract_nouns(df_insta_norm,1)\n",
    "nouns_minfreq_insta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns_minfreq_cafe = extract_nouns(df_cafe_norm,3)\n",
    "nouns_minfreq_cafe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-jc_fKQJtqV"
   },
   "source": [
    "## 5. ë¶ˆìš©ì–´ (stopwords) ì²˜ë¦¬\n",
    "\n",
    "ëª…ì‚¬ê°€ ì•„ë‹Œ ë‹¨ì–´ì™€ ë¶ˆí•„ìš”í•œ í•œê¸€ì ë‹¨ì–´ë“¤ì„ ì œê±°í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osZcRQ_ZC12t",
    "outputId": "d619f544-4cde-4855-d3b3-24a0b7841635"
   },
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2FPKx3vHN0O"
   },
   "source": [
    "stopwordsì—ì„œ 'ìš°ë¦¬', 'ì €í¬' ëŠ” ë¹¼ì•¼í•˜ëŠ”ê²ƒê°™ë‹¤. ì œë¡œì›¨ì´ìŠ¤íŠ¸ì˜ ê³µë™ì²´ ì„±ê²©ì„ ìœ ì§€í•˜ê¸°ìœ„í•´ì„œ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFr55EkuHifI"
   },
   "outputs": [],
   "source": [
    "stopwords.remove(['ìš°ë¦¬']); stopwords.remove(['ì €í¬']); stopwords.remove(['í•¨ê»˜']); len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY3nQ-PZC16B"
   },
   "outputs": [],
   "source": [
    "zerowaste_stopwords = ['ì œë¡œ','ì›¨','ì´ìŠ¤íŠ¸','ì´ìŠ¤í„°','ì œë¡œ ì›¨ì´ìŠ¤íŠ¸','ì œë¡œì›¨ì´ìŠ¤íŠ¸','zero waster',\\\n",
    "                       'ZERO WASTER', 'Zero Waster','zerowaster','ZEROWASTER','ZeroWaster',\n",
    "                       'zerowaste','ZEROWASTE','zero waste','ZERO WASTE','Zerowaste','Zero Waste',\\\n",
    "                       'ì œë¡œ ì›¨ì´ìŠ¤í„°','ì œë¡œì›¨ì´ìŠ¤í„°','ì›¨ì´ìŠ¤íŠ¸','ì›¨ì´ìŠ¤íŠ¸ ', ' ì›¨ì´ìŠ¤íŠ¸ ',' ì›¨ì´ìŠ¤íŠ¸',\\\n",
    "                       'í™˜ê²½','ì¹œí™˜ê²½','ë„ˆë¬´', 'ê·¸ë¦¬ê³ ', 'ì •ë§', 'ì´ë ‡ê²Œ', 'ìˆì–´ìš”', 'ë•Œë¬¸', \\\n",
    "                       'ì •ë„', 'ì¡°ê¸ˆ', 'ë¶„ë“¤', 'ì§„ì§œ', 'ëŒ€í•œ', 'ì´ë²ˆ', 'ê²½ìš°', 'ëŒ€ì‹ ', 'ê°€ì§€ê³ ',\\\n",
    "                       'ê·¸ë˜ì„œ','ì—„ì²­','ì•„ì§','ë•Œë¬¸ì—','ìœ„í•œ','ê·¸ëŸ°ë°','ê·¸ë ‡ê²Œ','ê²°êµ­','ê²ƒìœ¼ë¡œ',\\\n",
    "                       'ì´ê±°','ìˆì§€ë§Œ','ì‚¬ì´','ì‹¶ì–´ì„œ','ë‚˜ë¦„','ê·¸ê²ƒ',\\\n",
    "                       'ë•Œë¬¸ì—','ê°€ì¥','ê²ƒì„','ê´€ë ¨','ìˆëŠ”ë°','ê²ƒë„','ê·¼ë°','ë¬´ì—‡','ìˆë„ë¡','ë¬¼ë¡ ',\\\n",
    "                       'ë³´ë‹ˆ','ê²ƒì´ë‹¤','ë“±ì„','ë”ìš±','ë“±ë“±','ì´ê²ƒ','ê°™ì•„ì„œ','ìˆë‹¤ë©´','ìˆì—ˆì–´ìš”',\\\n",
    "                       'ìˆì—ˆëŠ”ë°','ê°™ìŠµë‹ˆë‹¤','ì•Šì„ê¹Œ','ì•Šì•„ë„','ì–´ëŠ','ì•Šê²Œ','ë„ˆë¬´ë‚˜','ì´ëŸ¬','ì´ê³³',\\\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-oOFxCUC184"
   },
   "outputs": [],
   "source": [
    "for word in zerowaste_stopwords:\n",
    "    stopwords.append(word)\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4svAeF5CFrH"
   },
   "outputs": [],
   "source": [
    "include = ['ë‚®', 'ë•…', 'ë°­', 'ê½ƒ', 'ëŒ', 'ë©‹','ë§›', 'í¼', 'ë¬¼', 'ë³•', 'ë¹›', 'ë´„', 'ìˆ²', 'ìƒˆ', 'ì‚°', 'ìˆ¨', 'ì‹¹', 'ì˜·', 'ì', 'ì°¨', 'í™', 'í˜']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¶ˆìš©ì–´ ì œê±°\n",
    "def eliminate_stopwords(nouns):\n",
    "    filtered=[]\n",
    "    for posting_nouns in nouns:\n",
    "        filtered_per_post=[]\n",
    "        for x in posting_nouns.keys():\n",
    "            if (len(x) > 1 or  x in include) and (x not in stopwords): \n",
    "                filtered_per_post.append((x,posting_nouns[x]))\n",
    "        filtered.append(filtered_per_post)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_blog = eliminate_stopwords(nouns_minfreq_blog)\n",
    "filtered_nouns_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_insta = eliminate_stopwords(nouns_minfreq_insta)\n",
    "filtered_nouns_insta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_cafe = eliminate_stopwords(nouns_minfreq_cafe)\n",
    "filtered_nouns_cafe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixx5wi8qBak_"
   },
   "source": [
    "## extractëœ nouns ì €ì¥(ë˜ëŠ” ë¯¸ë¦¬ ì €ì¥ëœ nounsë¶ˆëŸ¬ì˜¤ê¸°)\n",
    "\n",
    "ë½‘ì€ ëª…ì‚¬ë¥¼ pickleíŒŒì¼ë¡œ ì €ì¥í–ˆë‹¤. ê° í”Œë«í¼ë³„, ê²Œì‹œê¸€ë‚´ ì¶”ì¶œ ëª…ì‚¬ì™€ ë¹ˆë„ìˆ˜ ë° ì¶”ì¶œ ì ìˆ˜ê°€ ì €ì¥ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxK5coKLMw2I"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy1a5o6OMwzT"
   },
   "outputs": [],
   "source": [
    "file_to_store = open(\"./zerowaste/processed4200/filtered_soynlp_blog.pkl\", \"wb\")\n",
    "pickle.dump(filtered_nouns_blog,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./zerowaste/processed4200/filtered_soynlp_insta.pkl\", \"wb\")\n",
    "pickle.dump(filtered_nouns_insta,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./zerowaste/processed4200/filtered_soynlp_cafe.pkl\", \"wb\")\n",
    "pickle.dump(filtered_nouns_cafe,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì €ì¥ëœ pickleíŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./zerowaste/processed4200/filtered_soynlp_blog.pkl','rb') as input_file:\n",
    "    filtered_nouns_blog = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./zerowaste/processed4200/filtered_soynlp_insta.pkl','rb') as input_file:\n",
    "#     filtered_nouns_insta = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./zerowaste/processed4200/filtered_soynlp_cafe.pkl','rb') as input_file:\n",
    "    filtered_nouns_cafe = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì¸ìŠ¤íƒ€ê·¸ë¨ì€ ì „ì²˜ë¦¬ëœ hashtagê°€ ì¶”ê°€ëœ noun ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/hash_filtered_soynlp_insta.pkl','rb') as input_file:\n",
    "    hash_filtered_nouns_insta = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hash_filtered_nouns_insta[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_insta = hash_filtered_nouns_insta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_blog[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_cafe[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data transformation\n",
    "\n",
    "LDA ëª¨ë¸ì´ ì‚¬ìš©í•  dictionaryì™€ corpusë¥¼ ìƒì„±í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. dictionary ìƒì„±:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized(filtered_nouns):\n",
    "    nouns_list=[]\n",
    "    for posting in filtered_nouns:\n",
    "        nouns_per_posting=[]\n",
    "        if len(posting)>0:\n",
    "            for word in posting:\n",
    "                if len(word)>0:\n",
    "                    nouns_per_posting.append(word[0])\n",
    "        nouns_list.append(nouns_per_posting)\n",
    "    return nouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_doc_blog = get_tokenized(filtered_nouns_blog)\n",
    "token_doc_insta = get_tokenized(filtered_nouns_insta)\n",
    "token_doc_cafe = get_tokenized(filtered_nouns_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_doc = []\n",
    "token_doc.extend(token_doc_blog)\n",
    "token_doc.extend(token_doc_insta)\n",
    "token_doc.extend(token_doc_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\miniprojects\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(token_doc)\n",
    "#corpus = [dictionary.doc2bow(text) for text in token_doc]\n",
    "\n",
    "dictionary_blog = corpora.Dictionary(token_doc_blog)\n",
    "#corpus_blog = [dictionary_blog.doc2bow(text) for text in token_doc_blog]\n",
    "\n",
    "dictionary_insta = corpora.Dictionary(token_doc_insta)\n",
    "#corpus_insta = [dictionary_insta.doc2bow(text) for text in token_doc_insta]\n",
    "\n",
    "dictionary_cafe = corpora.Dictionary(token_doc_cafe)\n",
    "#corpus_cafe = [dictionary_cafe.doc2bow(text) for text in token_doc_cafe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8685\n",
      "6301\n",
      "2527\n",
      "3067\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "print(len(dictionary_blog))\n",
    "print(len(dictionary_insta))\n",
    "print(len(dictionary_cafe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ê³ ë¯¼'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ê³ ë¯¼'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_blog[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë§›'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_insta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìƒ´í‘¸ë°”'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_cafe[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. corpus ìƒì„±\n",
    "ì´ëŸ°..SOYNLPë¥¼ ì“°ë©´ corpusìƒì„±ì½”ë“œê°€ ë‹¬ë¦¬ì§„ë‹¤!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ê³ ì²´ì¹˜ì•½', NounScore(frequency=14, score=1.0)),\n",
       "  ('í”Œë¼ìŠ¤í‹±', NounScore(frequency=5, score=1.0)),\n",
       "  ('ê³ ë¯¼', NounScore(frequency=3, score=1.0)),\n",
       "  ('ì‚¬ìš©', NounScore(frequency=17, score=1.0)),\n",
       "  ('ê³µí•­', NounScore(frequency=3, score=1.0)),\n",
       "  ('ì£¼ë¬¸', NounScore(frequency=4, score=1.0)),\n",
       "  ('ì–‘ì¹˜', NounScore(frequency=6, score=1.0))],\n",
       " [('ì²œì—°ìˆ˜ì„¸ë¯¸', NounScore(frequency=10, score=1.0)),\n",
       "  ('í™”ì „ìƒíšŒ', NounScore(frequency=5, score=1.0)),\n",
       "  ('ë‰´ë¸”ë™', NounScore(frequency=3, score=1.0)),\n",
       "  ('ìˆ˜ì…ì‚°', NounScore(frequency=4, score=1.0)),\n",
       "  ('ì›í†µ', NounScore(frequency=4, score=1.0)),\n",
       "  ('ì‚¬ìš©', NounScore(frequency=8, score=1.0)),\n",
       "  ('êµ­ì‚°', NounScore(frequency=8, score=1.0)),\n",
       "  ('íŒë§¤', NounScore(frequency=5, score=1.0)),\n",
       "  ('ë…¸ë ¥', NounScore(frequency=3, score=1.0)),\n",
       "  ('ëª¨ìŠµ', NounScore(frequency=3, score=1.0)),\n",
       "  ('ì••ì¶•', NounScore(frequency=6, score=1.0)),\n",
       "  ('ë´‰ì œ', NounScore(frequency=5, score=1.0)),\n",
       "  ('ê°ì‚¬', NounScore(frequency=3, score=1.0)),\n",
       "  ('ì €í¬', NounScore(frequency=4, score=1.0))]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_nouns_blog[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_nouns_blog[0][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ê°¸ë˜', NounScore(frequency=1, score=0.5)),\n",
       "  ('ë§›', NounScore(frequency=2, score=1.0))],\n",
       " [('í…€ë¸”ëŸ¬', NounScore(frequency=2, score=1.0)),\n",
       "  ('ìŒë£Œ', NounScore(frequency=1, score=0.5))]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_nouns_insta[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ìƒ´í‘¸ë°”', NounScore(frequency=18, score=0.8571428571428571)),\n",
       "  ('ë¹„ëˆ„', NounScore(frequency=3, score=0.6))]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_nouns_cafe[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_for_words(nouns, dic):\n",
    "    posting_list=[]\n",
    "    for posting in nouns:\n",
    "        per_post=[]\n",
    "        for word in posting:\n",
    "            for i in range(len(dic)):\n",
    "                if dic[i] == word[0]:\n",
    "                    per_post.append(i)\n",
    "        posting_list.append(per_post)\n",
    "    return posting_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_for_words(nouns):\n",
    "    posting_list=[]\n",
    "    for posting in nouns:\n",
    "        per_post=[]\n",
    "        for word in posting:\n",
    "            per_post.append(word[1][0])\n",
    "        posting_list.append(per_post)\n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(index_list, freq_list):\n",
    "    posting_list=[]\n",
    "    for i in range(len(index_list)):\n",
    "        per_post = list(zip(index_list[i], freq_list[i]))\n",
    "        posting_list.append(per_post)\n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_blog = get_index_for_words(filtered_nouns_blog, dictionary_blog)\n",
    "freq_list_blog = get_freq_for_words(filtered_nouns_blog)\n",
    "corpus_blog = get_corpus(index_list_blog, freq_list_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 14), (6, 5), (0, 3), (3, 17), (2, 3), (5, 4), (4, 6)],\n",
       " [(17, 10),\n",
       "  (19, 5),\n",
       "  (10, 3),\n",
       "  (13, 4),\n",
       "  (15, 4),\n",
       "  (3, 8),\n",
       "  (8, 8),\n",
       "  (18, 5),\n",
       "  (9, 3),\n",
       "  (11, 3),\n",
       "  (14, 6),\n",
       "  (12, 5),\n",
       "  (7, 3),\n",
       "  (16, 4)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_blog[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_insta = get_index_for_words(filtered_nouns_insta, dictionary_insta)\n",
    "freq_list_insta = get_freq_for_words(filtered_nouns_insta)\n",
    "corpus_insta = get_corpus(index_list_insta, freq_list_insta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_cafe = get_index_for_words(filtered_nouns_cafe, dictionary_cafe)\n",
    "freq_list_cafe = get_freq_for_words(filtered_nouns_cafe)\n",
    "corpus_cafe = get_corpus(index_list_cafe, freq_list_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "corpus.extend(corpus_blog)\n",
    "corpus.extend(corpus_insta)\n",
    "corpus.extend(corpus_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11258\n",
      "4096\n",
      "3360\n",
      "3802\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(corpus_blog))\n",
    "print(len(corpus_insta))\n",
    "print(len(corpus_cafe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3. list of tokens ìƒì„±\n",
    "(soynlpì˜ ì¶”ì¶œë°©ì‹ì—ì„œë„ ë¬¸ì„œë‚´ì˜ ë‹¨ì–´ ìˆœì„œ ë³€ë™ì—†ì´ ë‹¨ì–´ ì¶”ì¶œí•˜ëŠ” ë°©ë²• ì°¾ì•„ë´ì•¼í•¨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab=[]\n",
    "for i in range(len(dictionary)):\n",
    "    my_vocab.append(dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_blog=[]\n",
    "for i in range(len(dictionary_blog)):\n",
    "    my_vocab_blog.append(dictionary_blog[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_insta=[]\n",
    "for i in range(len(dictionary_insta)):\n",
    "    my_vocab_insta.append(dictionary_insta[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_cafe=[]\n",
    "for i in range(len(dictionary_cafe)):\n",
    "    my_vocab_cafe.append(dictionary_cafe[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get DTM using my own vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜ì • í•„ìš”\n",
    "def get_dtm(df):\n",
    "    postings=[]\n",
    "    for idx in df.index:\n",
    "        post = df.loc[idx,'Content']\n",
    "    text=\n",
    "    vector = CountVectorizer()\n",
    "    dtm = vector.fit_transform(text).toarray()\n",
    "    return dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_blog\n",
    "\n",
    "vectorizer_blog = CountVectorizer()\n",
    "vectorizer_blog.fit_transform(my_vocab_blog)\n",
    "tf_blog = vectorizer_blog.transform(df_blog['Content'].tolist())\n",
    "dtm_blog = tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm instagram\n",
    "\n",
    "content_list_inta = df_insta\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(my_vocab_blog)\n",
    "tf = vectorizer.transform(df_blog['Content'].tolist())\n",
    "dtm = tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_cafe\n",
    "\n",
    "vectorizer_cafe = CountVectorizer()\n",
    "vectorizer_cafe.fit_transform(my_vocab_cafe)\n",
    "tf_cafe = vectorizer_cafe.transform(df_cafe['Content'].tolist())\n",
    "dtm_cafe = tf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save corpus, dictionary as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/corpus.pkl\", \"wb\")\n",
    "pickle.dump(corpus,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/corpus_blog.pkl\", \"wb\")\n",
    "pickle.dump(corpus_blog,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/corpus_insta.pkl\", \"wb\")\n",
    "pickle.dump(corpus_insta,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/corpus_cafe.pkl\", \"wb\")\n",
    "pickle.dump(corpus_cafe,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/dictionary.pkl\", \"wb\")\n",
    "pickle.dump(dictionary,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/dictionary_blog.pkl\", \"wb\")\n",
    "pickle.dump(dictionary_blog,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/dictionary_insta.pkl\", \"wb\")\n",
    "pickle.dump(dictionary_insta,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/dictionary_cafe.pkl\", \"wb\")\n",
    "pickle.dump(dictionary_cafe,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus, dictionary pickle file ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "pickle íŒŒì¼ì€ ìš°ë¦¬ êµ¬ê¸€ ê³µìœ í´ë”>sourcecode>pkl>soynlp í´ë”ì— ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/dictionary.pkl','rb') as input_file:\n",
    "    dictionary = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/dictionary_blog.pkl','rb') as input_file:\n",
    "    dictionary_blog = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/dictionary_insta.pkl','rb') as input_file:\n",
    "    dictionary_insta = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/dictionary_cafe.pkl','rb') as input_file:\n",
    "    dictionary_cafe = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/corpus.pkl','rb') as input_file:\n",
    "    corpus = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/corpus_blog.pkl','rb') as input_file:\n",
    "    corpus_blog = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/corpus_insta.pkl','rb') as input_file:\n",
    "    corpus_insta = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/corpus_cafe.pkl','rb') as input_file:\n",
    "    corpus_cafe = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LDA ëª¨ë¸ í›ˆë ¨ì‹œí‚¤ê¸°\n",
    "\n",
    "LDA modelì˜ ë§¤ê°œë³€ìˆ˜ ì¤‘ passë€? \n",
    "\n",
    "í›ˆë ¨ê³¼ì •ì˜ epochì™€ ê°™ì€ ê°œë…ì´ë‹¤.\n",
    "\n",
    "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be â€œepochsâ€. iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of â€œpassesâ€ and â€œiterationsâ€ high enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 initial guess model ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.058*\"ì‚¬ìš©\" + 0.022*\"ìˆ˜ì„¸ë¯¸\" + 0.017*\"ë¬¼\" + 0.015*\"ì²œì—°ì„¸ì œ\" + 0.013*\"ì„¤ê²†ì´\" + 0.011*\"ì†Œì°½\"')\n",
      "(1, '0.082*\"ê³„ë€\" + 0.054*\"ì‹ ë°œì •ë¦¬ëŒ€\" + 0.033*\"í¬ì¥\" + 0.026*\"ë§›\" + 0.023*\"ë…¸ë ¥\" + 0.023*\"ê³ ì–‘ì´\"')\n",
      "(2, '0.041*\"ë§¥ë½\" + 0.025*\"ë¬¼í’ˆ\" + 0.013*\"ê°•ì§€ë‚¨\" + 0.010*\"ìˆ˜ë¦¬ë§ˆì¼“\" + 0.009*\"í™œë™\" + 0.009*\"ë¦¬í•„\"')\n",
      "(3, '0.078*\"ì‚¬ìš©\" + 0.034*\"í”Œë¼ìŠ¤í‹±\" + 0.034*\"ì œí’ˆ\" + 0.026*\"ìŒì•…\" + 0.020*\"ë¹„ëˆ„\" + 0.016*\"ì¬í™œìš©\"')\n",
      "(4, '0.026*\"ë°€ëë©\" + 0.025*\"ìƒ´í‘¸ë°”\" + 0.018*\"ë§›ì§‘\" + 0.014*\"ì‚¬ìš©\" + 0.013*\"ë‘í”¼\" + 0.010*\"êµ¬ê³¨ë‹˜\"')\n"
     ]
    }
   ],
   "source": [
    "k = 5 #5ê°œì˜ í† í”½ ì„ì˜ ì§€ì •\n",
    "\n",
    "# í†µí•©\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, workers=4,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')\n",
    "\n",
    "topics = lda_model.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.055*\"ë§›\" + 0.028*\"ì¹´í˜\" + 0.017*\"ìˆ˜ì—…\" + 0.016*\"ìƒê°\" + 0.014*\"ì»¤í”¼\" + 0.014*\"ì˜¤ëŠ˜\"')\n",
      "(1, '0.068*\"í”Œë¼ìŠ¤í‹±\" + 0.061*\"ì‚¬ìš©\" + 0.031*\"ì¹«ì†”\" + 0.031*\"ì‹¤ì²œ\" + 0.026*\"ìš°ë¦¬\" + 0.024*\"ì§€êµ¬\"')\n",
      "(2, '0.047*\"ìƒê°\" + 0.027*\"ì‚¬ëŒ\" + 0.018*\"ë¬¼ê±´\" + 0.017*\"ì‹œê°„\" + 0.012*\"ì‹œì‘\" + 0.011*\"í™œë™\"')\n",
      "(3, '0.034*\"ì œí’ˆ\" + 0.014*\"ì‚¬ìš©\" + 0.012*\"ëšœê»‘\" + 0.011*\"ë¸Œëœë“œ\" + 0.010*\"ê°€ëŠ¥\" + 0.009*\"ì§„í–‰\"')\n",
      "(4, '0.157*\"ì‚¬ìš©\" + 0.045*\"ì œí’ˆ\" + 0.043*\"ë¹„ëˆ„\" + 0.036*\"ìˆ˜ì„¸ë¯¸\" + 0.021*\"ë¬¼\" + 0.020*\"ìƒ´í‘¸ë°”\"')\n"
     ]
    }
   ],
   "source": [
    "# ë¸”ë¡œê·¸\n",
    "lda_model_blog = gensim.models.LdaMulticore(corpus=corpus_blog,\n",
    "                                           id2word=dictionary_blog,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')\n",
    "\n",
    "topics = lda_model_blog.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.051*\"ìƒê°\" + 0.040*\"ë¹„ëˆ„\" + 0.032*\"ë§›\" + 0.023*\"í¬ì¥\" + 0.022*\"ë¬¼í‹°ìŠˆ\" + 0.013*\"ì‹œê°„\"')\n",
      "(1, '0.161*\"ì‚¬ìš©\" + 0.043*\"ì œí’ˆ\" + 0.030*\"ì‹¤ì²œ\" + 0.027*\"í”Œë¼ìŠ¤í‹±\" + 0.023*\"ì¬í™œìš©\" + 0.019*\"ì§€êµ¬\"')\n",
      "(2, '0.059*\"í•¨ê»˜\" + 0.048*\"í”¼ë¶€\" + 0.046*\"í¥ë¶„ìƒ‰\" + 0.015*\"ë†ì‚°ë¬¼\" + 0.015*\"ì—¬í–‰\" + 0.010*\"ì•°í”Œ\"')\n",
      "(3, '0.029*\"ìš°ë¦¬\" + 0.018*\"ì œí’ˆ\" + 0.013*\"ì—ì„¼ìŠ¤\" + 0.012*\"í”Œë¼ìŠ¤í‹±\" + 0.011*\"ìˆ˜ìœ \" + 0.010*\"ëŒ€ë‚˜ë¬´\"')\n",
      "(4, '0.037*\"ì˜¤ëŠ˜\" + 0.020*\"ìì—°\" + 0.017*\"ê±´ê°•\" + 0.017*\"ì“°ë ˆê¸°\" + 0.016*\"ìˆ˜ì„¸ë¯¸\" + 0.016*\"í…€ë¸”ëŸ¬\"')\n"
     ]
    }
   ],
   "source": [
    "# ì¸ìŠ¤íƒ€\n",
    "lda_model_insta = gensim.models.LdaMulticore(corpus=corpus_insta,\n",
    "                                           id2word=dictionary_insta,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')\n",
    "\n",
    "topics = lda_model_insta.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.087*\"í”Œë¼ìŠ¤í‹±\" + 0.058*\"ì¬í™œìš©\" + 0.027*\"ì œí’ˆ\" + 0.019*\"ê°€ëŠ¥\" + 0.017*\"ì‚¬ìš©\" + 0.015*\"ì–‘ë§\"')\n",
      "(1, '0.014*\"ê¸°ì—…\" + 0.013*\"ì½”ë¡œë‚˜\" + 0.012*\"ì‹œì¥\" + 0.009*\"íˆ¬ì\" + 0.009*\"ì£¼ì‹\" + 0.009*\"ê¸°ì\"')\n",
      "(2, '0.124*\"ì‚¬ìš©\" + 0.021*\"í¬ì¥\" + 0.019*\"ë¹„ë‹\" + 0.017*\"ì¹´í˜\" + 0.016*\"ìº í˜ì¸\" + 0.014*\"í™•ì¸\"')\n",
      "(3, '0.050*\"ìƒê°\" + 0.040*\"ìš°ë¦¬\" + 0.032*\"ì‹¤ì²œ\" + 0.025*\"í™œë™\" + 0.022*\"ì‚¬ëŒ\" + 0.016*\"ì§€êµ¬\"')\n",
      "(4, '0.069*\"ì‚¬ìš©\" + 0.058*\"ìƒí’ˆ\" + 0.035*\"ì¹«ì†”\" + 0.030*\"ë¹„ëˆ„\" + 0.030*\"ì œí’ˆ\" + 0.013*\"ì„¸ì œ\"')\n"
     ]
    }
   ],
   "source": [
    "# ì¹´í˜\n",
    "lda_model_cafe = gensim.models.LdaMulticore(corpus=corpus_cafe,\n",
    "                                           id2word=dictionary_cafe,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')\n",
    "\n",
    "topics = lda_model_cafe.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2. ì—°ìŠµìš©\n",
    "8ë²ˆìœ¼ë¡œ ë„˜ì–´ê°€ë©´ë©ë‹ˆë‹¤. ì—¬ê¸°ë¶€í„°ëŠ” ë§¤ê°œë³€ìˆ˜ë“¤ ë°”ê¿”ê°€ë©° ì‹œë„í•´ë³¸ê²ƒë“¤ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#í†µí•©, passes=20, í† í”½5ê°œì˜ ê° top6 ë‹¨ì–´ë“¤ì„ í™•ì¸\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = k, \\\n",
    "                                                id2word=dictionary, passes=20)\n",
    "topics = ldamodel.print_topics(num_words=6) \n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í†µí•©, passes=20, í† í”½5ê°œì˜ ê° top12 ë‹¨ì–´ë“¤ì„ í™•ì¸\n",
    "topics = ldamodel.print_topics(num_words=12) \n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#í†µí•©, passes = 80\n",
    "ldamodel_highpass = gensim.models.ldamodel.LdaModel(corpus, num_topics = k, \\\n",
    "                                                id2word=dictionary, passes=80)\n",
    "topics = ldamodel_highpass.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í†µí•©, LdaMulticore, alpha=0.01 ì§€ì •, passes=10\n",
    "lda_model_a = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=10, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)\n",
    "\n",
    "topics = lda_model_a.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë™ì¼ ë§¤ê°œë³€ìˆ˜ ì „ë‹¬í•˜ë©´ ë™ì¼í•œ ê²°ê³¼ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸ --> ì™„ì „ ë™ì¼í•˜ì§€ ì•Šì§€ë§Œ ë¹„ìŠ·í•¨.\n",
    "lda_model_a2 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=10, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)\n",
    "\n",
    "topics = lda_model_a2.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passesë¥¼ ë” ë†’íˆë©´, ë” ì¢‹ì€ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸ --> passes=10ì¼ë•Œë³´ë‹¤ êµ°ì§‘ëœ í† í”½ë“¤ì–´ ë” ëšœë ·í•˜ê²Œ ë‹¤ë¦„\n",
    "# í†µí•©, LdaMulticore, alpha=0.01 ì§€ì •, passes=80\n",
    "lda_model_a_highpass = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)\n",
    "\n",
    "topics = lda_model_a_highpass.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ë¸”ë¡œê·¸\n",
    "ldamodel_blog = gensim.models.ldamodel.LdaModel(corpus_blog, num_topics = k, \\\n",
    "                                                id2word=dictionary_blog, passes=20)\n",
    "topics_blog = ldamodel_blog.print_topics(num_words=6)\n",
    "for topic in topics_blog:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ìŠ¤íƒ€\n",
    "ldamodel_insta = gensim.models.ldamodel.LdaModel(corpus_insta, num_topics = k, \\\n",
    "                                                 id2word=dictionary_insta, passes=20)\n",
    "topics_insta = ldamodel_insta.print_topics(num_words=6)\n",
    "for topic in topics_insta:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¹´í˜\n",
    "ldamodel_cafe = gensim.models.ldamodel.LdaModel(corpus_cafe, num_topics = k, \\\n",
    "                                                id2word=dictionary_cafe, passes=20)\n",
    "topics_cafe = ldamodel_cafe.print_topics(num_words=6)\n",
    "for topic in topics_cafe:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ëª¨ë¸ topic coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì„ì˜ë¡œ ì§€ì •í•œ í† í”½ìˆ˜ê°€ 5ê°œì¼ë•Œì— í†µí•©ê³¼ ê° í”Œë«í¼ë³„ coherenceëŠ” ì–´ëŠìˆ˜ì¤€ì¼ê¹Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "co-occurrenceê¸°ë°˜ PMIë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì¸ coherence='c_uci'ì ìš©ì‹œ ì•„ì–˜ ë‹¤ë¥¸ scaleì˜ ê°’ì´ ë‚˜ì˜´."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model=ldamodel, texts=token_doc, \\\n",
    "#                                      dictionary=dictionary, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('í†µí•© Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=token_doc, \\\n",
    "                                     dictionary=dictionary, coherence='c_uci')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('í†µí•© Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_blog = CoherenceModel(model=ldamodel_blog, texts=token_doc_blog, \\\n",
    "                                     dictionary=dictionary_blog, coherence='c_uci')\n",
    "coherence_lda_blog = coherence_model_lda_blog.get_coherence()\n",
    "print('ë¸”ë¡œê·¸ Coherence Score: ', coherence_lda_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_insta = CoherenceModel(model=ldamodel_insta, texts=token_doc_insta, \\\n",
    "                                     dictionary=dictionary_insta, coherence='c_uci')\n",
    "coherence_lda_insta = coherence_model_lda_insta.get_coherence()\n",
    "print('ì¸ìŠ¤íƒ€ê·¸ë¨ Coherence Score: ', coherence_lda_insta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_cafe = CoherenceModel(model=ldamodel_cafe, texts=token_doc_cafe, \\\n",
    "                                     dictionary=dictionary_cafe, coherence='c_uci')\n",
    "coherence_lda_cafe = coherence_model_lda_cafe.get_coherence()\n",
    "print('ì¹´í˜ Coherence Score: ', coherence_lda_cafe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter íŠœë‹\n",
    "\n",
    "ë” ë¹ ë¥¸ implementationì„ ìœ„í•´ multicore ë²„ì ¼ì˜ ldamodelì¸ LdaMulticore()ì„ ì‚¬ìš©í•œë‹¤. LdaMulticoreë„ ê¸°ì¡´ ldamodel()ê³¼ ë™ì¼í•˜ë‹¤.\n",
    "\n",
    "ì°¸ê³ : https://radimrehurek.com/gensim/models/ldamodel.html , https://radimrehurek.com/gensim/models/ldamulticore.html#module-gensim.models.ldamulticore \n",
    "\n",
    "- Number of Topics (K)\n",
    "- Dirichlet hyperparameter **alpha**: Document-Topic Density - documentë‚´ì˜ topic concentration\n",
    "- Dirichlet hyperparameter **beta**: Word-Topic Density - topicë‚´ì˜ word mixtureê°€ ë‹¤ì–‘í•œ ì •ë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherenceê°’ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b, tokens):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokens, dictionary=dictionary, coherence='c_uci')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í†µí•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Hyperparameter tuning rangeì •ì˜:\n",
    "\n",
    "# Topics (kê°’) range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, \n",
    "                                                  k=k, a=a, b=b, tokens=token_doc)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('./LDAresults/lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./LDAresults/lda_tuning_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plot(df, Alpha, Beta):\n",
    "    df=df[df['Validation_Set']=='100% Corpus']\n",
    "    df=df[df['Alpha']==Alpha]\n",
    "    df=df[df['Beta']==Beta]\n",
    "    xAxis=df['Topics']\n",
    "    yAxis=df['Coherence']\n",
    "    \n",
    "    plt.plot(xAxis,yAxis)\n",
    "    plt.title('coherence for alpha={}, beta={}'.format(Alpha, Beta))\n",
    "    plt.xlabel('Topics')\n",
    "    plt.ylabel('Coherence')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy,0.01,0.31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy,0.31,0.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics8 = dfcopy[dfcopy['Topics']==8]\n",
    "df_topics8.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics6 = dfcopy[dfcopy['Topics']==6]\n",
    "df_topics6.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max8 = df_topics8['Coherence'].max()\n",
    "coherence_max8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved = ((coherence_max8-coherence_lda)/coherence_lda)*100\n",
    "percent_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max6 = df_topics6['Coherence'].max()\n",
    "coherence_max6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved = ((coherence_max6-coherence_lda)/coherence_lda)*100\n",
    "percent_improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì¸ìŠ¤íƒ€ê·¸ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Hyperparameter tuning rangeì •ì˜:\n",
    "\n",
    "# Topics (kê°’) range\n",
    "min_topics = 2\n",
    "max_topics = 14\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus_insta)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus_insta, int(num_of_docs*0.75)), \n",
    "               corpus_insta]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results_insta = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary_insta, \n",
    "                                                  k=k, a=a, b=b, tokens=token_doc_insta)\n",
    "                    # Save the model results\n",
    "                    model_results_insta['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results_insta['Topics'].append(k)\n",
    "                    model_results_insta['Alpha'].append(a)\n",
    "                    model_results_insta['Beta'].append(b)\n",
    "                    model_results_insta['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results_insta).to_csv('./LDAresults/lda_tuning_results_insta.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insta = pd.read_csv('./LDAresults/lda_tuning_results_insta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_insta = pd.DataFrame(model_results_insta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy_insta = df_insta.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_insta,0.31,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_insta,0.31,0.31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_insta,0.31,0.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ì¸ìŠ¤íƒ€ê·¸ë¨ì€ topics 10~13ì‚¬ì´ì—ì„œ alpha=0.31, beta=0.01ë¡œ ì ë‹¹í•œ í† í”½ìˆ˜ê°€ ê²°ì •ë˜ì—ˆìŒ.\n",
    "\n",
    "df_topics_insta13 = dfcopy_insta[dfcopy_insta['Topics']==13]\n",
    "df_topics_insta13.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_insta10 = dfcopy_insta[dfcopy_insta['Topics']==10]\n",
    "df_topics_insta10.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max_insta13 = df_topics_insta13['Coherence'].max()\n",
    "coherence_max_insta13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved_insta = ((coherence_max_insta13-coherence_lda_insta)/coherence_lda_insta)*100\n",
    "percent_improved_insta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max_insta10 = df_topics_insta10['Coherence'].max()\n",
    "coherence_max_insta10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved_insta10 = ((coherence_max_insta10-coherence_lda_insta)/coherence_lda_insta)*100\n",
    "percent_improved_insta10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë¸”ë¡œê·¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Hyperparameter tuning rangeì •ì˜:\n",
    "\n",
    "# Topics (kê°’) range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus_blog)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus_blog, int(num_of_docs*0.75)), \n",
    "               corpus_blog]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results_blog = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary_blog, \n",
    "                                                  k=k, a=a, b=b, tokens=token_doc_blog)\n",
    "                    # Save the model results\n",
    "                    model_results_blog['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results_blog['Topics'].append(k)\n",
    "                    model_results_blog['Alpha'].append(a)\n",
    "                    model_results_blog['Beta'].append(b)\n",
    "                    model_results_blog['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results_blog).to_csv('./LDAresults/lda_tuning_results_blog.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blog = pd.read_csv('./LDAresults/lda_tuning_results_blog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_blog = pd.DataFrame(model_results_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy_blog = df_blog.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_blog,0.01,0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_blog,0.01,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_insta,0.31,0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_blog10 = dfcopy_blog[dfcopy_blog['Topics']==10]\n",
    "df_topics_blog10.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_blog8 = dfcopy_blog[dfcopy_blog['Topics']==8]\n",
    "df_topics_blog8.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_blog5 = dfcopy_blog[dfcopy_blog['Topics']==5]\n",
    "df_topics_blog5.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max_blog10 = df_topics_blog10['Coherence'].max()\n",
    "coherence_max_blog10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved_blog10 = ((coherence_max_blog10-coherence_lda_blog)/coherence_lda_blog)*100\n",
    "percent_improved_blog10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> ê·¸ë˜ì„œ ë¸”ë¡œê·¸ topic modeling ì§ˆë¬¸: k=10, 8, 5ì¤‘ ì–´ë–¤ê²ƒì„ ê³¨ë¼ì•¼í• ê¹Œ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì¹´í˜ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Hyperparameter tuning rangeì •ì˜:\n",
    "\n",
    "# Topics (kê°’) range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus_cafe)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus_cafe, int(num_of_docs*0.75)), \n",
    "               corpus_cafe]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results_cafe = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary_cafe, \n",
    "                                                  k=k, a=a, b=b, tokens=token_doc_cafe)\n",
    "                    # Save the model results\n",
    "                    model_results_cafe ['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results_cafe ['Topics'].append(k)\n",
    "                    model_results_cafe ['Alpha'].append(a)\n",
    "                    model_results_cafe ['Beta'].append(b)\n",
    "                    model_results_cafe ['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results_cafe ).to_csv('./LDAresults/lda_tuning_results_cafe.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cafe = pd.read_csv('./LDAresults/lda_tuning_results_cafe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cafe = pd.DataFrame(model_results_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cafe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy_cafe = df_cafe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_cafe,0.31,0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_cafe,0.31,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_cafe,0.31,0.31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_cafe,0.31,0.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_cafe10 = dfcopy_cafe[dfcopy_cafe['Topics']==10]\n",
    "df_topics_cafe10.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_cafe9 = dfcopy_cafe[dfcopy_cafe['Topics']==9]\n",
    "df_topics_cafe9.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max_cafe9 = df_topics_cafe9['Coherence'].max()\n",
    "coherence_max_cafe9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved_cafe9 = ((coherence_max_cafe9-coherence_lda_cafe)/coherence_lda_cafe)*100\n",
    "percent_improved_cafe9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. final LDA ëª¨ë¸ í™•ë³´ ë° ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í†µí•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=8\n",
    "lda_model_final = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.31,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = gensimvis.prepare(lda_model_final, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_all_8.html'\n",
    "pyLDAvis.save_html(vis, pyldavis_html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=6\n",
    "lda_model_final6 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.31,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final6.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis2 = gensimvis.prepare(lda_model_final6, corpus, dictionary)\n",
    "pyLDAvis.display(vis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_all_6.html'\n",
    "pyLDAvis.save_html(vis2, pyldavis_html_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¸ìŠ¤íƒ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "lda_model_final_insta = gensim.models.LdaMulticore(corpus=corpus_insta,\n",
    "                                           id2word=dictionary_insta,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.31,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final_insta.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis_insta = gensimvis.prepare(lda_model_final_insta, corpus_insta, dictionary_insta)\n",
    "pyLDAvis.display(vis_insta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_insta.html'\n",
    "pyLDAvis.save_html(vis_insta, pyldavis_html_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¸”ë¡œê·¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "lda_model_final_blog = gensim.models.LdaMulticore(corpus=corpus_blog,\n",
    "                                           id2word=dictionary_blog,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final_blog.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis_blog = gensimvis.prepare(lda_model_final_blog, corpus_blog, dictionary_blog)\n",
    "pyLDAvis.display(vis_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_blog.html'\n",
    "pyLDAvis.save_html(vis_blog, pyldavis_html_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¹´í˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=9\n",
    "lda_model_final_cafe = gensim.models.LdaMulticore(corpus=corpus_cafe,\n",
    "                                           id2word=dictionary_cafe,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.61,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final_cafe.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis_cafe = gensimvis.prepare(lda_model_final_cafe, corpus_cafe, dictionary_cafe)\n",
    "pyLDAvis.display(vis_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_cafe.html'\n",
    "pyLDAvis.save_html(vis_cafe, pyldavis_html_path) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SNS_txtdata_preprocessing_and_topicModeling2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "miniprojects",
   "language": "python",
   "name": "miniprojects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
