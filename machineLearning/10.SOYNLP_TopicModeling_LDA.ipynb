{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 및 토큰화 과정 step by step\n",
    "\n",
    "\n",
    "### Soynlp (‘딥러닝을 이용한 자연어 처리 입문’도서 참고)\n",
    "\n",
    "Non-null만 추출 -> 광고성 글 제거 -> soynlp normalizer -> soyspacing -> 정규식 적용(한글 및 영어 문자만 남기고 나머지 제거 + url형태 문자열 제거) -> LRNounExtractor_v2 명사추출 -> 불용어 제외\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZ5wursIwIFd"
   },
   "source": [
    "## 0. 환경 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 광고 제거까지 정제된 데이터 로드\n",
    "\n",
    "Non-null만 추출 -> 광고성 글 제거 까지 처리되어 csv저장된 data 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Like</th>\n",
       "      <th>Content</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>SNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://blog.naver.com/lsy_sweet/222457159636</td>\n",
       "      <td>2021. 8. 15. 9:58</td>\n",
       "      <td>12.0</td>\n",
       "      <td>안녕하세요, 친환경 살림을 하며 환경 보존을 위해 노력하는 단순이 입니다. 그동안 ...</td>\n",
       "      <td>#고체치약 #제로웨이스트 #고체치약본티 #본티치약 #Vontee</td>\n",
       "      <td>blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://blog.naver.com/newblack76/222468647865</td>\n",
       "      <td>2021. 8. 15. 9:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>모두들 안녕하셨어요~ 입추가 지나고 말복이 지나니 이젠 바람이 틀려진듯해요^^ 금방...</td>\n",
       "      <td>#국산천연수세미 #제로웨이스트선물 #국내산천연수세미 #북어표수세미 #잘라쓰는수세미 ...</td>\n",
       "      <td>blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://blog.naver.com/somcandy117/222460526981</td>\n",
       "      <td>2021. 8. 15. 9:10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>오빠가 유리빨대의 딱딱한 질감이 싫다고 해서 100퍼 자연주의 풀빨대를 사봤다 배송...</td>\n",
       "      <td>#풀빨대 #자연빨대 #제로웨이스트 #생분해빨대 #베트남풀빨대 #친환경빨대 #환경보호...</td>\n",
       "      <td>blog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               URL                Date  Like  \\\n",
       "0    https://blog.naver.com/lsy_sweet/222457159636   2021. 8. 15. 9:58  12.0   \n",
       "1   https://blog.naver.com/newblack76/222468647865   2021. 8. 15. 9:49   NaN   \n",
       "2  https://blog.naver.com/somcandy117/222460526981   2021. 8. 15. 9:10   2.0   \n",
       "\n",
       "                                             Content  \\\n",
       "0  안녕하세요, 친환경 살림을 하며 환경 보존을 위해 노력하는 단순이 입니다. 그동안 ...   \n",
       "1  모두들 안녕하셨어요~ 입추가 지나고 말복이 지나니 이젠 바람이 틀려진듯해요^^ 금방...   \n",
       "2  오빠가 유리빨대의 딱딱한 질감이 싫다고 해서 100퍼 자연주의 풀빨대를 사봤다 배송...   \n",
       "\n",
       "                                             Hashtag   SNS  \n",
       "0                #고체치약 #제로웨이스트 #고체치약본티 #본티치약 #Vontee  blog  \n",
       "1  #국산천연수세미 #제로웨이스트선물 #국내산천연수세미 #북어표수세미 #잘라쓰는수세미 ...  blog  \n",
       "2  #풀빨대 #자연빨대 #제로웨이스트 #생분해빨대 #베트남풀빨대 #친환경빨대 #환경보호...  blog  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blog = pd.read_csv('./zerowaste/processed4200/blog_non-null_no-ads_4200.csv')\n",
    "df_blog.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Like</th>\n",
       "      <th>Content</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>SNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vege_yony</td>\n",
       "      <td>2021-08-15 23:59:36+00:00</td>\n",
       "      <td>49</td>\n",
       "      <td>- 그릭요거트 다들 식사로 먹지만 간식으로 먹는 사람 나야 나- 카페 조감 비건그릭...</td>\n",
       "      <td>['#비건그릭요거트', '#쑥브라우니', '#비건쑥디저트']</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eco_kapegg</td>\n",
       "      <td>2021-08-15 23:45:31+00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>⠀ 더운날🥲 음료필수장착해야되쥬 중부사부소 조과장님의 텀블러 용기내👍👏 ⠀ 텀블러에...</td>\n",
       "      <td>['#용기내', '#제로웨이스트', '#환경보호', '#텀블러', '#캠페인', '...</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eco_kapegg</td>\n",
       "      <td>2021-08-15 23:42:02+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>⠀ 중부사무소 서대리님의 주말 장보기🤗 미리 장바구니를 챙겨서 봉투사용을 줄였습니다...</td>\n",
       "      <td>['#제로웨이스트', '#캠페인', '#장바구니', '#일회용품줄이기', '#봉투줄...</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          URL                       Date Like  \\\n",
       "0   vege_yony  2021-08-15 23:59:36+00:00   49   \n",
       "1  eco_kapegg  2021-08-15 23:45:31+00:00   16   \n",
       "2  eco_kapegg  2021-08-15 23:42:02+00:00   14   \n",
       "\n",
       "                                             Content  \\\n",
       "0  - 그릭요거트 다들 식사로 먹지만 간식으로 먹는 사람 나야 나- 카페 조감 비건그릭...   \n",
       "1  ⠀ 더운날🥲 음료필수장착해야되쥬 중부사부소 조과장님의 텀블러 용기내👍👏 ⠀ 텀블러에...   \n",
       "2  ⠀ 중부사무소 서대리님의 주말 장보기🤗 미리 장바구니를 챙겨서 봉투사용을 줄였습니다...   \n",
       "\n",
       "                                             Hashtag        SNS  \n",
       "0                  ['#비건그릭요거트', '#쑥브라우니', '#비건쑥디저트']  instagram  \n",
       "1  ['#용기내', '#제로웨이스트', '#환경보호', '#텀블러', '#캠페인', '...  instagram  \n",
       "2  ['#제로웨이스트', '#캠페인', '#장바구니', '#일회용품줄이기', '#봉투줄...  instagram  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_insta = pd.read_csv('./zerowaste/processed4200/insta_non-null_no-ads_4200.csv')\n",
    "df_insta.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Like</th>\n",
       "      <th>Content</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>SNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cafe.naver.com/applestore99/1704?art=a...</td>\n",
       "      <td>2021.08.15. 23:25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>​​​써보고 평이 가장 좋았던 갓성비 샴푸바 이구요. 가격후기별 순위모음​​​  &lt;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cafe.naver.com/myomahealing/151141?art...</td>\n",
       "      <td>2021.08.15. 23:01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>글쓰기 전 기본 정보가 있는 '공부합시다' 게시판을 확인하시고, 중복되는 문의글이 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://cafe.naver.com/donggubat0/72?art=aW50Z...</td>\n",
       "      <td>2021.08.15. 22:39</td>\n",
       "      <td>2.0</td>\n",
       "      <td>동구밭과 첫 인연을 맺게 해준, 그 첫 인연이후 쭉 저의 최애템은 '올바른 설거지 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL               Date  Like  \\\n",
       "0  https://cafe.naver.com/applestore99/1704?art=a...  2021.08.15. 23:25   0.0   \n",
       "1  https://cafe.naver.com/myomahealing/151141?art...  2021.08.15. 23:01   3.0   \n",
       "2  https://cafe.naver.com/donggubat0/72?art=aW50Z...  2021.08.15. 22:39   2.0   \n",
       "\n",
       "                                             Content Hashtag   SNS  \n",
       "0  ​​​써보고 평이 가장 좋았던 갓성비 샴푸바 이구요. 가격후기별 순위모음​​​  <...     NaN  cafe  \n",
       "1  글쓰기 전 기본 정보가 있는 '공부합시다' 게시판을 확인하시고, 중복되는 문의글이 ...     NaN  cafe  \n",
       "2  동구밭과 첫 인연을 맺게 해준, 그 첫 인연이후 쭉 저의 최애템은 '올바른 설거지 ...     NaN  cafe  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cafe = pd.read_csv('./zerowaste/processed4200/cafe_non-null_no-ads_4200.csv')\n",
    "df_cafe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200\n",
      "4200\n",
      "4200\n"
     ]
    }
   ],
   "source": [
    "print(len(df_blog))\n",
    "print(len(df_insta))\n",
    "print(len(df_cafe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2rSCVquxIFZ"
   },
   "source": [
    "## 2. Soyspacing & SOYNLP normalizer 처리\n",
    "\n",
    "#### soyspacing\n",
    "\n",
    "띄어쓰기 에러 처리 및 이모티콘, 반복 글자 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soyspacing\n",
    "print(soyspacing.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soyspacing.countbase import RuleDict, CountSpace\n",
    "\n",
    "model2 = CountSpace()\n",
    "model2.load_model('./soyspacing/demo_model/test.model', json_format=False)\n",
    "\n",
    "verbose=False\n",
    "mc = 10  # min_count\n",
    "ft = 0.3 # force_abs_threshold\n",
    "nt =-0.3 # nonspace_threshold\n",
    "st = 0.3 # space_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_spacing(sent):\n",
    "    sent_corrected, tags = model2.correct(\n",
    "        sent,\n",
    "        verbose=verbose,\n",
    "        force_abs_threshold=ft,\n",
    "        nonspace_threshold=nt,\n",
    "        space_threshold=st,\n",
    "        min_count=mc\n",
    "    )\n",
    "    return sent_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blog['Content'] = df_blog['Content'].apply(lambda x: fix_spacing(x))\n",
    "df_insta['Content'] = df_insta['Content'].apply(lambda x: fix_spacing(x))\n",
    "df_cafe['Content'] = df_cafe['Content'].apply(lambda x: fix_spacing(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqkP1CPH1XJl"
   },
   "source": [
    "#### SOYNLP의 Normalizer\n",
    "\n",
    "대화 데이터, 댓글 데이터에 등장하는 반복되는 이모티콘의 정리 및 한글, 혹은 텍스트만 남기기 위한 함수를 제공합니다.\n",
    "\n",
    "예시)\n",
    "\n",
    "emoticon_normalize('ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ쿠ㅜㅜㅜㅜㅜㅜ', num_repeats=3)\n",
    "\n",
    "결과:  'ㅋㅋㅋㅜㅜㅜ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XINPHyxp1Zp-"
   },
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akal6JXg1whG"
   },
   "outputs": [],
   "source": [
    "def normalize_all(df):\n",
    "    for idx in df.index:\n",
    "        df.loc[idx,'Content'] = emoticon_normalize(df.loc[idx,'Content'], num_repeats=3) \n",
    "        df.loc[idx,'Content'] = repeat_normalize(df.loc[idx,'Content'], num_repeats=2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mp2iX9Ry1Zl6",
    "outputId": "ce6b11e8-84cf-41af-8a38-3e75a90572fa"
   },
   "outputs": [],
   "source": [
    "df_blog_norm = normalize_all(df_blog)\n",
    "df_insta_norm = normalize_all(df_insta)\n",
    "df_cafe_norm = normalize_all(df_cafe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 게시글별 문장 리스트화 + 정규식 적용\n",
    "\n",
    "게시글 내용을 문장단위로 나누어 리스트화 함\n",
    "\n",
    "URL 제외, 한글과 영어만 남도록 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sentences(text, punc):\n",
    "    # syonlp noun extractor 사용을 위해 문장사이 doublespace 삽입\n",
    "    sentences=[]; onesentence=\"\"\n",
    "    for ele in text:\n",
    "        onesentence += ele\n",
    "        if ele in punc:\n",
    "            sentences.append(onesentence+\"  \")\n",
    "            onesentence=\"\"\n",
    "    return sentences\n",
    "            \n",
    "def regular_expression(text): \n",
    "    # url제거\n",
    "    url = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "    pattern_url = re.compile(url)\n",
    "    tmp = pattern_url.sub('', text)\n",
    "    # regex추출 규칙: 영어 또는 띄어 쓰기(1 개)를 포함한 한글\n",
    "    pattern = re.compile('[^a-zA-Z| ㄱ-ㅣ 가-힣]')  \n",
    "    result = pattern.sub('', tmp)  # 위에 설정한 \"hangul\"규칙을 \"text\"에 적용(.sub)시킴\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sep_regex(text,punct):\n",
    "    sentences = separate_sentences(text,punct)\n",
    "    reg_sentences=[]\n",
    "    for s in sentences:\n",
    "        reg_sentences.append(regular_expression(s))\n",
    "    return reg_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEXSUu7HpOSi",
    "outputId": "ec09cfb7-dac0-4c1c-e30a-d93ac6d7abc7"
   },
   "outputs": [],
   "source": [
    "#punctuation = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "punctuation = '''!.?~'''\n",
    "\n",
    "df_blog_norm['Content'] = df_blog_norm['Content'].apply(lambda x: apply_sep_regex(x, punctuation))\n",
    "df_insta_norm['Content'] = df_insta_norm['Content'].apply(lambda x: apply_sep_regex(x, punctuation))\n",
    "df_cafe_norm['Content'] = df_cafe_norm['Content'].apply(lambda x: apply_sep_regex(x, punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elminate_empty_lines(text):\n",
    "    non_empty=[]\n",
    "    for line in text:\n",
    "        if len(line)>0 and line!=' ': non_empty.append(line)\n",
    "    return non_empty         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blog_norm['Content'] = df_blog_norm['Content'].apply(lambda x: elminate_empty_lines(x))\n",
    "df_insta_norm['Content'] = df_insta_norm['Content'].apply(lambda x: elminate_empty_lines(x))\n",
    "df_cafe_norm['Content'] = df_cafe_norm['Content'].apply(lambda x: elminate_empty_lines(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG762IAH7v4H"
   },
   "source": [
    "## 4. 토큰화 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zrs1KPLOGBqQ"
   },
   "source": [
    "### SOYNLP의 Noun Extractor(v.2) 활용 \n",
    "\n",
    "soynlp는 품사 태깅, 단어 토큰화들을 제공함.\n",
    "비지도 학습으로 단어 토큰화 진행 - 데이터에서 자주 등장하는 단어들 (신조어 포함) 단어로 분석함.\n",
    "\n",
    "soynlp=0.0.46+ 에서는 명사 추출기 version 2 를 제공합니다. 이전 버전의 명사 추출의 정확성과 합성명사 인식 능력, 출력되는 정보의 오류를 수정한 버전입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 각 게시글별 명사(+게시글별 빈도수) 추출\n",
    "\n",
    "df['Content']의 각 게시글을 문장단위로 나뉜 elements를 가진 list이다.\n",
    "\n",
    "sents = df_blog.loc[idx,'Content']\n",
    "\n",
    "sents를 noun_extractor에 넣어서 각 게시글별 명사와 빈도수 및 score를 추출한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(df,freq):\n",
    "    nouns=[]\n",
    "    for idx in df.index:\n",
    "        sents = df.loc[idx,'Content']\n",
    "        try:\n",
    "            nouns_per_posting = noun_extractor.train_extract(\n",
    "            sents,\n",
    "            min_noun_frequency=freq)\n",
    "            nouns.append(nouns_per_posting)\n",
    "        except:\n",
    "            pass\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minfreq=3 설정 이유 - 서론,본론,결론에 적어도 한번씩 등장하는 단어라면 3+ 예상함.\n",
    "nouns_minfreq_blog = extract_nouns(df_blog_norm,3)\n",
    "nouns_minfreq_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_minfreq_insta = extract_nouns(df_insta_norm,1)\n",
    "nouns_minfreq_insta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns_minfreq_cafe = extract_nouns(df_cafe_norm,3)\n",
    "nouns_minfreq_cafe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-jc_fKQJtqV"
   },
   "source": [
    "## 5. 불용어 (stopwords) 처리\n",
    "\n",
    "명사가 아닌 단어와 불필요한 한글자 단어들을 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osZcRQ_ZC12t",
    "outputId": "d619f544-4cde-4855-d3b3-24a0b7841635"
   },
   "outputs": [],
   "source": [
    "stopwords=[]\n",
    "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2FPKx3vHN0O"
   },
   "source": [
    "stopwords에서 '우리', '저희' 는 빼야하는것같다. 제로웨이스트의 공동체 성격을 유지하기위해서."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFr55EkuHifI"
   },
   "outputs": [],
   "source": [
    "stopwords.remove(['우리']); stopwords.remove(['저희']); stopwords.remove(['함께']); len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY3nQ-PZC16B"
   },
   "outputs": [],
   "source": [
    "zerowaste_stopwords = ['제로','웨','이스트','이스터','제로 웨이스트','제로웨이스트','zero waster',\\\n",
    "                       'ZERO WASTER', 'Zero Waster','zerowaster','ZEROWASTER','ZeroWaster',\n",
    "                       'zerowaste','ZEROWASTE','zero waste','ZERO WASTE','Zerowaste','Zero Waste',\\\n",
    "                       '제로 웨이스터','제로웨이스터','웨이스트','웨이스트 ', ' 웨이스트 ',' 웨이스트',\\\n",
    "                       '환경','친환경','너무', '그리고', '정말', '이렇게', '있어요', '때문', \\\n",
    "                       '정도', '조금', '분들', '진짜', '대한', '이번', '경우', '대신', '가지고',\\\n",
    "                       '그래서','엄청','아직','때문에','위한','그런데','그렇게','결국','것으로',\\\n",
    "                       '이거','있지만','사이','싶어서','나름','그것',\\\n",
    "                       '때문에','가장','것을','관련','있는데','것도','근데','무엇','있도록','물론',\\\n",
    "                       '보니','것이다','등을','더욱','등등','이것','같아서','있다면','있었어요',\\\n",
    "                       '있었는데','같습니다','않을까','않아도','어느','않게','너무나','이러','이곳',\\\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i-oOFxCUC184"
   },
   "outputs": [],
   "source": [
    "for word in zerowaste_stopwords:\n",
    "    stopwords.append(word)\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4svAeF5CFrH"
   },
   "outputs": [],
   "source": [
    "include = ['낮', '땅', '밭', '꽃', '돌', '멋','맛', '폼', '물', '볕', '빛', '봄', '숲', '새', '산', '숨', '싹', '옷', '잎', '차', '흙', '힘']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "def eliminate_stopwords(nouns):\n",
    "    filtered=[]\n",
    "    for posting_nouns in nouns:\n",
    "        filtered_per_post=[]\n",
    "        for x in posting_nouns.keys():\n",
    "            if (len(x) > 1 or  x in include) and (x not in stopwords): \n",
    "                filtered_per_post.append((x,posting_nouns[x]))\n",
    "        filtered.append(filtered_per_post)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_blog = eliminate_stopwords(nouns_minfreq_blog)\n",
    "filtered_nouns_blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_insta = eliminate_stopwords(nouns_minfreq_insta)\n",
    "filtered_nouns_insta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_cafe = eliminate_stopwords(nouns_minfreq_cafe)\n",
    "filtered_nouns_cafe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixx5wi8qBak_"
   },
   "source": [
    "## extract된 nouns 저장(또는 미리 저장된 nouns불러오기)\n",
    "\n",
    "뽑은 명사를 pickle파일로 저장했다. 각 플랫폼별, 게시글내 추출 명사와 빈도수 및 추출 점수가 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxK5coKLMw2I"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy1a5o6OMwzT"
   },
   "outputs": [],
   "source": [
    "file_to_store = open(\"./zerowaste/processed4200/filtered_soynlp_blog.pkl\", \"wb\")\n",
    "pickle.dump(filtered_nouns_blog,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./zerowaste/processed4200/filtered_soynlp_insta.pkl\", \"wb\")\n",
    "pickle.dump(filtered_nouns_insta,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./zerowaste/processed4200/filtered_soynlp_cafe.pkl\", \"wb\")\n",
    "pickle.dump(filtered_nouns_cafe,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장된 pickle파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./zerowaste/processed4200/filtered_soynlp_blog.pkl','rb') as input_file:\n",
    "    filtered_nouns_blog = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./zerowaste/processed4200/filtered_soynlp_insta.pkl','rb') as input_file:\n",
    "#     filtered_nouns_insta = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./zerowaste/processed4200/filtered_soynlp_cafe.pkl','rb') as input_file:\n",
    "    filtered_nouns_cafe = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인스타그램은 전처리된 hashtag가 추가된 noun 리스트를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/hash_filtered_soynlp_insta.pkl','rb') as input_file:\n",
    "    hash_filtered_nouns_insta = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hash_filtered_nouns_insta[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_insta = hash_filtered_nouns_insta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_blog[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nouns_cafe[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data transformation\n",
    "\n",
    "LDA 모델이 사용할 dictionary와 corpus를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1. dictionary 생성:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized(filtered_nouns):\n",
    "    nouns_list=[]\n",
    "    for posting in filtered_nouns:\n",
    "        nouns_per_posting=[]\n",
    "        if len(posting)>0:\n",
    "            for word in posting:\n",
    "                if len(word)>0:\n",
    "                    nouns_per_posting.append(word[0])\n",
    "        nouns_list.append(nouns_per_posting)\n",
    "    return nouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_doc_blog = get_tokenized(filtered_nouns_blog)\n",
    "token_doc_insta = get_tokenized(filtered_nouns_insta)\n",
    "token_doc_cafe = get_tokenized(filtered_nouns_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_doc = []\n",
    "token_doc.extend(token_doc_blog)\n",
    "token_doc.extend(token_doc_insta)\n",
    "token_doc.extend(token_doc_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\miniprojects\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(token_doc)\n",
    "#corpus = [dictionary.doc2bow(text) for text in token_doc]\n",
    "\n",
    "dictionary_blog = corpora.Dictionary(token_doc_blog)\n",
    "#corpus_blog = [dictionary_blog.doc2bow(text) for text in token_doc_blog]\n",
    "\n",
    "dictionary_insta = corpora.Dictionary(token_doc_insta)\n",
    "#corpus_insta = [dictionary_insta.doc2bow(text) for text in token_doc_insta]\n",
    "\n",
    "dictionary_cafe = corpora.Dictionary(token_doc_cafe)\n",
    "#corpus_cafe = [dictionary_cafe.doc2bow(text) for text in token_doc_cafe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8685\n",
      "6301\n",
      "2527\n",
      "3067\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "print(len(dictionary_blog))\n",
    "print(len(dictionary_insta))\n",
    "print(len(dictionary_cafe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'고민'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'고민'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_blog[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'맛'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_insta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'샴푸바'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_cafe[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. corpus 생성\n",
    "이런..SOYNLP를 쓰면 corpus생성코드가 달리진다!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('고체치약', NounScore(frequency=14, score=1.0)),\n",
       "  ('플라스틱', NounScore(frequency=5, score=1.0)),\n",
       "  ('고민', NounScore(frequency=3, score=1.0)),\n",
       "  ('사용', NounScore(frequency=17, score=1.0)),\n",
       "  ('공항', NounScore(frequency=3, score=1.0)),\n",
       "  ('주문', NounScore(frequency=4, score=1.0)),\n",
       "  ('양치', NounScore(frequency=6, score=1.0))],\n",
       " [('천연수세미', NounScore(frequency=10, score=1.0)),\n",
       "  ('화전상회', NounScore(frequency=5, score=1.0)),\n",
       "  ('뉴블랙', NounScore(frequency=3, score=1.0)),\n",
       "  ('수입산', NounScore(frequency=4, score=1.0)),\n",
       "  ('원통', NounScore(frequency=4, score=1.0)),\n",
       "  ('사용', NounScore(frequency=8, score=1.0)),\n",
       "  ('국산', NounScore(frequency=8, score=1.0)),\n",
       "  ('판매', NounScore(frequency=5, score=1.0)),\n",
       "  ('노력', NounScore(frequency=3, score=1.0)),\n",
       "  ('모습', NounScore(frequency=3, score=1.0)),\n",
       "  ('압축', NounScore(frequency=6, score=1.0)),\n",
       "  ('봉제', NounScore(frequency=5, score=1.0)),\n",
       "  ('감사', NounScore(frequency=3, score=1.0)),\n",
       "  ('저희', NounScore(frequency=4, score=1.0))]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_nouns_blog[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_nouns_blog[0][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('갸또', NounScore(frequency=1, score=0.5)),\n",
       "  ('맛', NounScore(frequency=2, score=1.0))],\n",
       " [('텀블러', NounScore(frequency=2, score=1.0)),\n",
       "  ('음료', NounScore(frequency=1, score=0.5))]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_nouns_insta[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('샴푸바', NounScore(frequency=18, score=0.8571428571428571)),\n",
       "  ('비누', NounScore(frequency=3, score=0.6))]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_nouns_cafe[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_for_words(nouns, dic):\n",
    "    posting_list=[]\n",
    "    for posting in nouns:\n",
    "        per_post=[]\n",
    "        for word in posting:\n",
    "            for i in range(len(dic)):\n",
    "                if dic[i] == word[0]:\n",
    "                    per_post.append(i)\n",
    "        posting_list.append(per_post)\n",
    "    return posting_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_for_words(nouns):\n",
    "    posting_list=[]\n",
    "    for posting in nouns:\n",
    "        per_post=[]\n",
    "        for word in posting:\n",
    "            per_post.append(word[1][0])\n",
    "        posting_list.append(per_post)\n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(index_list, freq_list):\n",
    "    posting_list=[]\n",
    "    for i in range(len(index_list)):\n",
    "        per_post = list(zip(index_list[i], freq_list[i]))\n",
    "        posting_list.append(per_post)\n",
    "    return posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_blog = get_index_for_words(filtered_nouns_blog, dictionary_blog)\n",
    "freq_list_blog = get_freq_for_words(filtered_nouns_blog)\n",
    "corpus_blog = get_corpus(index_list_blog, freq_list_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 14), (6, 5), (0, 3), (3, 17), (2, 3), (5, 4), (4, 6)],\n",
       " [(17, 10),\n",
       "  (19, 5),\n",
       "  (10, 3),\n",
       "  (13, 4),\n",
       "  (15, 4),\n",
       "  (3, 8),\n",
       "  (8, 8),\n",
       "  (18, 5),\n",
       "  (9, 3),\n",
       "  (11, 3),\n",
       "  (14, 6),\n",
       "  (12, 5),\n",
       "  (7, 3),\n",
       "  (16, 4)]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_blog[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_insta = get_index_for_words(filtered_nouns_insta, dictionary_insta)\n",
    "freq_list_insta = get_freq_for_words(filtered_nouns_insta)\n",
    "corpus_insta = get_corpus(index_list_insta, freq_list_insta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_cafe = get_index_for_words(filtered_nouns_cafe, dictionary_cafe)\n",
    "freq_list_cafe = get_freq_for_words(filtered_nouns_cafe)\n",
    "corpus_cafe = get_corpus(index_list_cafe, freq_list_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "corpus.extend(corpus_blog)\n",
    "corpus.extend(corpus_insta)\n",
    "corpus.extend(corpus_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11258\n",
      "4096\n",
      "3360\n",
      "3802\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(corpus_blog))\n",
    "print(len(corpus_insta))\n",
    "print(len(corpus_cafe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-3. list of tokens 생성\n",
    "(soynlp의 추출방식에서도 문서내의 단어 순서 변동없이 단어 추출하는 방법 찾아봐야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab=[]\n",
    "for i in range(len(dictionary)):\n",
    "    my_vocab.append(dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_blog=[]\n",
    "for i in range(len(dictionary_blog)):\n",
    "    my_vocab_blog.append(dictionary_blog[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_insta=[]\n",
    "for i in range(len(dictionary_insta)):\n",
    "    my_vocab_insta.append(dictionary_insta[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vocab_cafe=[]\n",
    "for i in range(len(dictionary_cafe)):\n",
    "    my_vocab_cafe.append(dictionary_cafe[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get DTM using my own vocab list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정 필요\n",
    "def get_dtm(df):\n",
    "    postings=[]\n",
    "    for idx in df.index:\n",
    "        post = df.loc[idx,'Content']\n",
    "    text=\n",
    "    vector = CountVectorizer()\n",
    "    dtm = vector.fit_transform(text).toarray()\n",
    "    return dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_blog\n",
    "\n",
    "vectorizer_blog = CountVectorizer()\n",
    "vectorizer_blog.fit_transform(my_vocab_blog)\n",
    "tf_blog = vectorizer_blog.transform(df_blog['Content'].tolist())\n",
    "dtm_blog = tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm instagram\n",
    "\n",
    "content_list_inta = df_insta\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(my_vocab_blog)\n",
    "tf = vectorizer.transform(df_blog['Content'].tolist())\n",
    "dtm = tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_cafe\n",
    "\n",
    "vectorizer_cafe = CountVectorizer()\n",
    "vectorizer_cafe.fit_transform(my_vocab_cafe)\n",
    "tf_cafe = vectorizer_cafe.transform(df_cafe['Content'].tolist())\n",
    "dtm_cafe = tf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save corpus, dictionary as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/corpus.pkl\", \"wb\")\n",
    "pickle.dump(corpus,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/corpus_blog.pkl\", \"wb\")\n",
    "pickle.dump(corpus_blog,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/corpus_insta.pkl\", \"wb\")\n",
    "pickle.dump(corpus_insta,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/corpus_cafe.pkl\", \"wb\")\n",
    "pickle.dump(corpus_cafe,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/dictionary.pkl\", \"wb\")\n",
    "pickle.dump(dictionary,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/dictionary_blog.pkl\", \"wb\")\n",
    "pickle.dump(dictionary_blog,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/dictionary_insta.pkl\", \"wb\")\n",
    "pickle.dump(dictionary_insta,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"./soynlp/dictionary_cafe.pkl\", \"wb\")\n",
    "pickle.dump(dictionary_cafe,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus, dictionary pickle file 불러오기\n",
    "\n",
    "pickle 파일은 우리 구글 공유폴더>sourcecode>pkl>soynlp 폴더에 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/dictionary.pkl','rb') as input_file:\n",
    "    dictionary = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/dictionary_blog.pkl','rb') as input_file:\n",
    "    dictionary_blog = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/dictionary_insta.pkl','rb') as input_file:\n",
    "    dictionary_insta = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/dictionary_cafe.pkl','rb') as input_file:\n",
    "    dictionary_cafe = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/corpus.pkl','rb') as input_file:\n",
    "    corpus = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/corpus_blog.pkl','rb') as input_file:\n",
    "    corpus_blog = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/corpus_insta.pkl','rb') as input_file:\n",
    "    corpus_insta = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./soynlp/corpus_cafe.pkl','rb') as input_file:\n",
    "    corpus_cafe = pickle.load(input_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LDA 모델 훈련시키기\n",
    "\n",
    "LDA model의 매개변수 중 pass란? \n",
    "\n",
    "훈련과정의 epoch와 같은 개념이다.\n",
    "\n",
    "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be “epochs”. iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of “passes” and “iterations” high enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 initial guess model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.058*\"사용\" + 0.022*\"수세미\" + 0.017*\"물\" + 0.015*\"천연세제\" + 0.013*\"설겆이\" + 0.011*\"소창\"')\n",
      "(1, '0.082*\"계란\" + 0.054*\"신발정리대\" + 0.033*\"포장\" + 0.026*\"맛\" + 0.023*\"노력\" + 0.023*\"고양이\"')\n",
      "(2, '0.041*\"맥락\" + 0.025*\"물품\" + 0.013*\"강지남\" + 0.010*\"수리마켓\" + 0.009*\"활동\" + 0.009*\"리필\"')\n",
      "(3, '0.078*\"사용\" + 0.034*\"플라스틱\" + 0.034*\"제품\" + 0.026*\"음악\" + 0.020*\"비누\" + 0.016*\"재활용\"')\n",
      "(4, '0.026*\"밀랍랩\" + 0.025*\"샴푸바\" + 0.018*\"맛집\" + 0.014*\"사용\" + 0.013*\"두피\" + 0.010*\"구골님\"')\n"
     ]
    }
   ],
   "source": [
    "k = 5 #5개의 토픽 임의 지정\n",
    "\n",
    "# 통합\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, workers=4,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')\n",
    "\n",
    "topics = lda_model.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.055*\"맛\" + 0.028*\"카페\" + 0.017*\"수업\" + 0.016*\"생각\" + 0.014*\"커피\" + 0.014*\"오늘\"')\n",
      "(1, '0.068*\"플라스틱\" + 0.061*\"사용\" + 0.031*\"칫솔\" + 0.031*\"실천\" + 0.026*\"우리\" + 0.024*\"지구\"')\n",
      "(2, '0.047*\"생각\" + 0.027*\"사람\" + 0.018*\"물건\" + 0.017*\"시간\" + 0.012*\"시작\" + 0.011*\"활동\"')\n",
      "(3, '0.034*\"제품\" + 0.014*\"사용\" + 0.012*\"뚜껑\" + 0.011*\"브랜드\" + 0.010*\"가능\" + 0.009*\"진행\"')\n",
      "(4, '0.157*\"사용\" + 0.045*\"제품\" + 0.043*\"비누\" + 0.036*\"수세미\" + 0.021*\"물\" + 0.020*\"샴푸바\"')\n"
     ]
    }
   ],
   "source": [
    "# 블로그\n",
    "lda_model_blog = gensim.models.LdaMulticore(corpus=corpus_blog,\n",
    "                                           id2word=dictionary_blog,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')\n",
    "\n",
    "topics = lda_model_blog.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.051*\"생각\" + 0.040*\"비누\" + 0.032*\"맛\" + 0.023*\"포장\" + 0.022*\"물티슈\" + 0.013*\"시간\"')\n",
      "(1, '0.161*\"사용\" + 0.043*\"제품\" + 0.030*\"실천\" + 0.027*\"플라스틱\" + 0.023*\"재활용\" + 0.019*\"지구\"')\n",
      "(2, '0.059*\"함께\" + 0.048*\"피부\" + 0.046*\"흥분색\" + 0.015*\"농산물\" + 0.015*\"여행\" + 0.010*\"앰플\"')\n",
      "(3, '0.029*\"우리\" + 0.018*\"제품\" + 0.013*\"에센스\" + 0.012*\"플라스틱\" + 0.011*\"수유\" + 0.010*\"대나무\"')\n",
      "(4, '0.037*\"오늘\" + 0.020*\"자연\" + 0.017*\"건강\" + 0.017*\"쓰레기\" + 0.016*\"수세미\" + 0.016*\"텀블러\"')\n"
     ]
    }
   ],
   "source": [
    "# 인스타\n",
    "lda_model_insta = gensim.models.LdaMulticore(corpus=corpus_insta,\n",
    "                                           id2word=dictionary_insta,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')\n",
    "\n",
    "topics = lda_model_insta.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.087*\"플라스틱\" + 0.058*\"재활용\" + 0.027*\"제품\" + 0.019*\"가능\" + 0.017*\"사용\" + 0.015*\"양말\"')\n",
      "(1, '0.014*\"기업\" + 0.013*\"코로나\" + 0.012*\"시장\" + 0.009*\"투자\" + 0.009*\"주식\" + 0.009*\"기자\"')\n",
      "(2, '0.124*\"사용\" + 0.021*\"포장\" + 0.019*\"비닐\" + 0.017*\"카페\" + 0.016*\"캠페인\" + 0.014*\"확인\"')\n",
      "(3, '0.050*\"생각\" + 0.040*\"우리\" + 0.032*\"실천\" + 0.025*\"활동\" + 0.022*\"사람\" + 0.016*\"지구\"')\n",
      "(4, '0.069*\"사용\" + 0.058*\"상품\" + 0.035*\"칫솔\" + 0.030*\"비누\" + 0.030*\"제품\" + 0.013*\"세제\"')\n"
     ]
    }
   ],
   "source": [
    "# 카페\n",
    "lda_model_cafe = gensim.models.LdaMulticore(corpus=corpus_cafe,\n",
    "                                           id2word=dictionary_cafe,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')\n",
    "\n",
    "topics = lda_model_cafe.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7-2. 연습용\n",
    "8번으로 넘어가면됩니다. 여기부터는 매개변수들 바꿔가며 시도해본것들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#통합, passes=20, 토픽5개의 각 top6 단어들을 확인\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = k, \\\n",
    "                                                id2word=dictionary, passes=20)\n",
    "topics = ldamodel.print_topics(num_words=6) \n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통합, passes=20, 토픽5개의 각 top12 단어들을 확인\n",
    "topics = ldamodel.print_topics(num_words=12) \n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#통합, passes = 80\n",
    "ldamodel_highpass = gensim.models.ldamodel.LdaModel(corpus, num_topics = k, \\\n",
    "                                                id2word=dictionary, passes=80)\n",
    "topics = ldamodel_highpass.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통합, LdaMulticore, alpha=0.01 지정, passes=10\n",
    "lda_model_a = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=10, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)\n",
    "\n",
    "topics = lda_model_a.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동일 매개변수 전달하면 동일한 결과나오는지 확인 --> 완전 동일하지 않지만 비슷함.\n",
    "lda_model_a2 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=10, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)\n",
    "\n",
    "topics = lda_model_a2.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passes를 더 높히면, 더 좋은 결과가 나오는지 확인 --> passes=10일때보다 군집된 토픽들어 더 뚜렷하게 다름\n",
    "# 통합, LdaMulticore, alpha=0.01 지정, passes=80\n",
    "lda_model_a_highpass = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=80, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)\n",
    "\n",
    "topics = lda_model_a_highpass.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 블로그\n",
    "ldamodel_blog = gensim.models.ldamodel.LdaModel(corpus_blog, num_topics = k, \\\n",
    "                                                id2word=dictionary_blog, passes=20)\n",
    "topics_blog = ldamodel_blog.print_topics(num_words=6)\n",
    "for topic in topics_blog:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인스타\n",
    "ldamodel_insta = gensim.models.ldamodel.LdaModel(corpus_insta, num_topics = k, \\\n",
    "                                                 id2word=dictionary_insta, passes=20)\n",
    "topics_insta = ldamodel_insta.print_topics(num_words=6)\n",
    "for topic in topics_insta:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카페\n",
    "ldamodel_cafe = gensim.models.ldamodel.LdaModel(corpus_cafe, num_topics = k, \\\n",
    "                                                id2word=dictionary_cafe, passes=20)\n",
    "topics_cafe = ldamodel_cafe.print_topics(num_words=6)\n",
    "for topic in topics_cafe:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 모델 topic coherence score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의로 지정한 토픽수가 5개일때에 통합과 각 플랫폼별 coherence는 어느수준일까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "co-occurrence기반 PMI를 사용하는 방식인 coherence='c_uci'적용시 아얘 다른 scale의 값이 나옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compute Coherence Score\n",
    "# coherence_model_lda = CoherenceModel(model=ldamodel, texts=token_doc, \\\n",
    "#                                      dictionary=dictionary, coherence='c_v')\n",
    "# coherence_lda = coherence_model_lda.get_coherence()\n",
    "# print('통합 Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=token_doc, \\\n",
    "                                     dictionary=dictionary, coherence='c_uci')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('통합 Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_blog = CoherenceModel(model=ldamodel_blog, texts=token_doc_blog, \\\n",
    "                                     dictionary=dictionary_blog, coherence='c_uci')\n",
    "coherence_lda_blog = coherence_model_lda_blog.get_coherence()\n",
    "print('블로그 Coherence Score: ', coherence_lda_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_insta = CoherenceModel(model=ldamodel_insta, texts=token_doc_insta, \\\n",
    "                                     dictionary=dictionary_insta, coherence='c_uci')\n",
    "coherence_lda_insta = coherence_model_lda_insta.get_coherence()\n",
    "print('인스타그램 Coherence Score: ', coherence_lda_insta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda_cafe = CoherenceModel(model=ldamodel_cafe, texts=token_doc_cafe, \\\n",
    "                                     dictionary=dictionary_cafe, coherence='c_uci')\n",
    "coherence_lda_cafe = coherence_model_lda_cafe.get_coherence()\n",
    "print('카페 Coherence Score: ', coherence_lda_cafe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter 튜닝\n",
    "\n",
    "더 빠른 implementation을 위해 multicore 버젼의 ldamodel인 LdaMulticore()을 사용한다. LdaMulticore도 기존 ldamodel()과 동일하다.\n",
    "\n",
    "참고: https://radimrehurek.com/gensim/models/ldamodel.html , https://radimrehurek.com/gensim/models/ldamulticore.html#module-gensim.models.ldamulticore \n",
    "\n",
    "- Number of Topics (K)\n",
    "- Dirichlet hyperparameter **alpha**: Document-Topic Density - document내의 topic concentration\n",
    "- Dirichlet hyperparameter **beta**: Word-Topic Density - topic내의 word mixture가 다양한 정도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence값을 계산하는 함수\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b, tokens):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=20,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=tokens, dictionary=dictionary, coherence='c_uci')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Hyperparameter tuning range정의:\n",
    "\n",
    "# Topics (k값) range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, \n",
    "                                                  k=k, a=a, b=b, tokens=token_doc)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('./LDAresults/lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./LDAresults/lda_tuning_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_plot(df, Alpha, Beta):\n",
    "    df=df[df['Validation_Set']=='100% Corpus']\n",
    "    df=df[df['Alpha']==Alpha]\n",
    "    df=df[df['Beta']==Beta]\n",
    "    xAxis=df['Topics']\n",
    "    yAxis=df['Coherence']\n",
    "    \n",
    "    plt.plot(xAxis,yAxis)\n",
    "    plt.title('coherence for alpha={}, beta={}'.format(Alpha, Beta))\n",
    "    plt.xlabel('Topics')\n",
    "    plt.ylabel('Coherence')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy,0.01,0.31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy,0.31,0.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics8 = dfcopy[dfcopy['Topics']==8]\n",
    "df_topics8.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics6 = dfcopy[dfcopy['Topics']==6]\n",
    "df_topics6.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max8 = df_topics8['Coherence'].max()\n",
    "coherence_max8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved = ((coherence_max8-coherence_lda)/coherence_lda)*100\n",
    "percent_improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max6 = df_topics6['Coherence'].max()\n",
    "coherence_max6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved = ((coherence_max6-coherence_lda)/coherence_lda)*100\n",
    "percent_improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인스타그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Hyperparameter tuning range정의:\n",
    "\n",
    "# Topics (k값) range\n",
    "min_topics = 2\n",
    "max_topics = 14\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus_insta)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus_insta, int(num_of_docs*0.75)), \n",
    "               corpus_insta]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results_insta = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary_insta, \n",
    "                                                  k=k, a=a, b=b, tokens=token_doc_insta)\n",
    "                    # Save the model results\n",
    "                    model_results_insta['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results_insta['Topics'].append(k)\n",
    "                    model_results_insta['Alpha'].append(a)\n",
    "                    model_results_insta['Beta'].append(b)\n",
    "                    model_results_insta['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results_insta).to_csv('./LDAresults/lda_tuning_results_insta.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insta = pd.read_csv('./LDAresults/lda_tuning_results_insta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_insta = pd.DataFrame(model_results_insta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy_insta = df_insta.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_insta,0.31,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_insta,0.31,0.31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_insta,0.31,0.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 인스타그램은 topics 10~13사이에서 alpha=0.31, beta=0.01로 적당한 토픽수가 결정되었음.\n",
    "\n",
    "df_topics_insta13 = dfcopy_insta[dfcopy_insta['Topics']==13]\n",
    "df_topics_insta13.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_insta10 = dfcopy_insta[dfcopy_insta['Topics']==10]\n",
    "df_topics_insta10.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max_insta13 = df_topics_insta13['Coherence'].max()\n",
    "coherence_max_insta13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved_insta = ((coherence_max_insta13-coherence_lda_insta)/coherence_lda_insta)*100\n",
    "percent_improved_insta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max_insta10 = df_topics_insta10['Coherence'].max()\n",
    "coherence_max_insta10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved_insta10 = ((coherence_max_insta10-coherence_lda_insta)/coherence_lda_insta)*100\n",
    "percent_improved_insta10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 블로그"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Hyperparameter tuning range정의:\n",
    "\n",
    "# Topics (k값) range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus_blog)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus_blog, int(num_of_docs*0.75)), \n",
    "               corpus_blog]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results_blog = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary_blog, \n",
    "                                                  k=k, a=a, b=b, tokens=token_doc_blog)\n",
    "                    # Save the model results\n",
    "                    model_results_blog['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results_blog['Topics'].append(k)\n",
    "                    model_results_blog['Alpha'].append(a)\n",
    "                    model_results_blog['Beta'].append(b)\n",
    "                    model_results_blog['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results_blog).to_csv('./LDAresults/lda_tuning_results_blog.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blog = pd.read_csv('./LDAresults/lda_tuning_results_blog.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_blog = pd.DataFrame(model_results_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy_blog = df_blog.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_blog,0.01,0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_blog,0.01,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_insta,0.31,0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_blog10 = dfcopy_blog[dfcopy_blog['Topics']==10]\n",
    "df_topics_blog10.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_blog8 = dfcopy_blog[dfcopy_blog['Topics']==8]\n",
    "df_topics_blog8.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_blog5 = dfcopy_blog[dfcopy_blog['Topics']==5]\n",
    "df_topics_blog5.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max_blog10 = df_topics_blog10['Coherence'].max()\n",
    "coherence_max_blog10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved_blog10 = ((coherence_max_blog10-coherence_lda_blog)/coherence_lda_blog)*100\n",
    "percent_improved_blog10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> 그래서 블로그 topic modeling 질문: k=10, 8, 5중 어떤것을 골라야할까??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 카페 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Hyperparameter tuning range정의:\n",
    "\n",
    "# Topics (k값) range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus_cafe)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus_cafe, int(num_of_docs*0.75)), \n",
    "               corpus_cafe]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results_cafe = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary_cafe, \n",
    "                                                  k=k, a=a, b=b, tokens=token_doc_cafe)\n",
    "                    # Save the model results\n",
    "                    model_results_cafe ['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results_cafe ['Topics'].append(k)\n",
    "                    model_results_cafe ['Alpha'].append(a)\n",
    "                    model_results_cafe ['Beta'].append(b)\n",
    "                    model_results_cafe ['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results_cafe ).to_csv('./LDAresults/lda_tuning_results_cafe.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cafe = pd.read_csv('./LDAresults/lda_tuning_results_cafe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cafe = pd.DataFrame(model_results_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cafe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcopy_cafe = df_cafe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_cafe,0.31,0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_cafe,0.31,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_cafe,0.31,0.31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_plot(dfcopy_cafe,0.31,0.61)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_cafe10 = dfcopy_cafe[dfcopy_cafe['Topics']==10]\n",
    "df_topics_cafe10.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_topics_cafe9 = dfcopy_cafe[dfcopy_cafe['Topics']==9]\n",
    "df_topics_cafe9.sort_values(by=\"Coherence\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_max_cafe9 = df_topics_cafe9['Coherence'].max()\n",
    "coherence_max_cafe9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_improved_cafe9 = ((coherence_max_cafe9-coherence_lda_cafe)/coherence_lda_cafe)*100\n",
    "percent_improved_cafe9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. final LDA 모델 확보 및 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=8\n",
    "lda_model_final = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.31,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = gensimvis.prepare(lda_model_final, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_all_8.html'\n",
    "pyLDAvis.save_html(vis, pyldavis_html_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=6\n",
    "lda_model_final6 = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.31,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final6.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis2 = gensimvis.prepare(lda_model_final6, corpus, dictionary)\n",
    "pyLDAvis.display(vis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_all_6.html'\n",
    "pyLDAvis.save_html(vis2, pyldavis_html_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인스타"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "lda_model_final_insta = gensim.models.LdaMulticore(corpus=corpus_insta,\n",
    "                                           id2word=dictionary_insta,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.31,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final_insta.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis_insta = gensimvis.prepare(lda_model_final_insta, corpus_insta, dictionary_insta)\n",
    "pyLDAvis.display(vis_insta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_insta.html'\n",
    "pyLDAvis.save_html(vis_insta, pyldavis_html_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 블로그"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "lda_model_final_blog = gensim.models.LdaMulticore(corpus=corpus_blog,\n",
    "                                           id2word=dictionary_blog,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.01,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final_blog.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis_blog = gensimvis.prepare(lda_model_final_blog, corpus_blog, dictionary_blog)\n",
    "pyLDAvis.display(vis_blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_blog.html'\n",
    "pyLDAvis.save_html(vis_blog, pyldavis_html_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카페"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=9\n",
    "lda_model_final_cafe = gensim.models.LdaMulticore(corpus=corpus_cafe,\n",
    "                                           id2word=dictionary_cafe,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,#num of docs to be used each training\n",
    "                                           passes=100, #num of passes thru corpus during training\n",
    "                                           alpha=0.61,\n",
    "                                           eta='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda_model_final_cafe.print_topics(num_words=6)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis_cafe = gensimvis.prepare(lda_model_final_cafe, corpus_cafe, dictionary_cafe)\n",
    "pyLDAvis.display(vis_cafe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyldavis_html_path='./LDAresults/LDAvis_cafe.html'\n",
    "pyLDAvis.save_html(vis_cafe, pyldavis_html_path) "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SNS_txtdata_preprocessing_and_topicModeling2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "miniprojects",
   "language": "python",
   "name": "miniprojects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
