{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZ5wursIwIFd"
   },
   "source": [
    "### 환경 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install g++ openjdk-8-jdk python3-dev python3-pip curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FUuqSGul54iE"
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xs4G6mpVwz5n"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOvKxmWPwOjG"
   },
   "source": [
    "### 클롤링 데이터 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-KVWQwSWEeQ",
    "outputId": "2f65edcc-5c74-4fa1-ba27-f4dc1ff8a704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "x4k2ZWAGYVjM",
    "outputId": "cd8f5c5e-5404-4670-dba1-f50c97fe599b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "Ns9e0HoxOG_G",
    "outputId": "4a1d84c2-f682-429d-9722-a81ecb3a191f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5583\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Like</th>\n",
       "      <th>Content</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>SNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://blog.naver.com/lsy_sweet/222457159636</td>\n",
       "      <td>2021. 8. 15. 9:58</td>\n",
       "      <td>12.0</td>\n",
       "      <td>안녕하세요, 친환경 살림을 하며 환경 보존을 위해 노력하는 단순이 입니다. 그동안 ...</td>\n",
       "      <td>#고체치약 #제로웨이스트 #고체치약본티 #본티치약 #Vontee</td>\n",
       "      <td>blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://blog.naver.com/newblack76/222468647865</td>\n",
       "      <td>2021. 8. 15. 9:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>모두들 안녕하셨어요~ 입추가 지나고 말복이 지나니 이젠 바람이 틀려진듯해요^^ 금방...</td>\n",
       "      <td>#국산천연수세미 #제로웨이스트선물 #국내산천연수세미 #북어표수세미 #잘라쓰는수세미 ...</td>\n",
       "      <td>blog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://blog.naver.com/somcandy117/222460526981</td>\n",
       "      <td>2021. 8. 15. 9:10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>오빠가 유리빨대의 딱딱한 질감이 싫다고 해서 100퍼 자연주의 풀빨대를 사봤다 배송...</td>\n",
       "      <td>#풀빨대 #자연빨대 #제로웨이스트 #생분해빨대 #베트남풀빨대 #친환경빨대 #환경보호...</td>\n",
       "      <td>blog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               URL  ...   SNS\n",
       "0    https://blog.naver.com/lsy_sweet/222457159636  ...  blog\n",
       "1   https://blog.naver.com/newblack76/222468647865  ...  blog\n",
       "2  https://blog.naver.com/somcandy117/222460526981  ...  blog\n",
       "\n",
       "[3 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('gdrive/My Drive/dataset_zerowaste/blog.csv')\n",
    "df1['SNS'] = 'blog'\n",
    "print(len(df1['Content']))\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "gYCu6RDdSKLZ",
    "outputId": "f6069ccd-804b-44b4-b041-186c4db46b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Like</th>\n",
       "      <th>Content</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>SNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vege_yony</td>\n",
       "      <td>2021-08-15 23:59:36+00:00</td>\n",
       "      <td>49</td>\n",
       "      <td>- 그릭요거트 다들 식사로 먹지만 간식으로 먹는 사람 나야 나- 카페 조감 비건그릭...</td>\n",
       "      <td>['#비건그릭요거트', '#쑥브라우니', '#비건쑥디저트']</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eco_kapegg</td>\n",
       "      <td>2021-08-15 23:45:31+00:00</td>\n",
       "      <td>16</td>\n",
       "      <td>⠀ 더운날🥲 음료필수장착해야되쥬 중부사부소 조과장님의 텀블러 용기내👍👏 ⠀ 텀블러에...</td>\n",
       "      <td>['#용기내', '#제로웨이스트', '#환경보호', '#텀블러', '#캠페인', '...</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eco_kapegg</td>\n",
       "      <td>2021-08-15 23:42:02+00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>⠀ 중부사무소 서대리님의 주말 장보기🤗 미리 장바구니를 챙겨서 봉투사용을 줄였습니다...</td>\n",
       "      <td>['#제로웨이스트', '#캠페인', '#장바구니', '#일회용품줄이기', '#봉투줄...</td>\n",
       "      <td>instagram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          URL  ...        SNS\n",
       "0   vege_yony  ...  instagram\n",
       "1  eco_kapegg  ...  instagram\n",
       "2  eco_kapegg  ...  instagram\n",
       "\n",
       "[3 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('gdrive/My Drive/dataset_zerowaste/instagram.csv')\n",
    "df2 = df2.drop('Unnamed: 0', axis=1)\n",
    "df2 = df2.rename(columns={\"id\": \"URL\", \"date\": \"Date\", \"like\": \"Like\", \"text\":\"Content\", \"hashtag\":\"Hashtag\"})\n",
    "df2['SNS'] = 'instagram'\n",
    "print(len(df2['Content']))\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "-16qqgft2dMm",
    "outputId": "6391388b-84d0-4f82-e57b-907bfd0f009c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6301\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "      <th>Like</th>\n",
       "      <th>Content</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>SNS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://cafe.naver.com/applestore99/1704?art=a...</td>\n",
       "      <td>2021.08.15. 23:25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>​​​써보고 평이 가장 좋았던 갓성비 샴푸바 이구요. 가격후기별 순위모음​​​  &lt;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://cafe.naver.com/myomahealing/151141?art...</td>\n",
       "      <td>2021.08.15. 23:01</td>\n",
       "      <td>3.0</td>\n",
       "      <td>글쓰기 전 기본 정보가 있는 '공부합시다' 게시판을 확인하시고, 중복되는 문의글이 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://cafe.naver.com/donggubat0/72?art=aW50Z...</td>\n",
       "      <td>2021.08.15. 22:39</td>\n",
       "      <td>2.0</td>\n",
       "      <td>동구밭과 첫 인연을 맺게 해준, 그 첫 인연이후 쭉 저의 최애템은 '올바른 설거지 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cafe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  ...   SNS\n",
       "0  https://cafe.naver.com/applestore99/1704?art=a...  ...  cafe\n",
       "1  https://cafe.naver.com/myomahealing/151141?art...  ...  cafe\n",
       "2  https://cafe.naver.com/donggubat0/72?art=aW50Z...  ...  cafe\n",
       "\n",
       "[3 rows x 6 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('gdrive/My Drive/dataset_zerowaste/cafe.csv')\n",
    "df3['SNS'] = 'cafe'\n",
    "print(len(df3['Content']))\n",
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KHsFpszZeUc",
    "outputId": "335e5484-64da-433e-e206-d46d153bf2ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5583 entries, 0 to 5582\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   URL      5583 non-null   object \n",
      " 1   Date     5382 non-null   object \n",
      " 2   Like     5190 non-null   float64\n",
      " 3   Content  5303 non-null   object \n",
      " 4   Hashtag  4913 non-null   object \n",
      " 5   SNS      5583 non-null   object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 261.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HI3cFESCZhCD",
    "outputId": "1122e3e5-4a1e-4e05-cc31-3e2fcbad7a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4500 entries, 0 to 4499\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   URL      4500 non-null   object\n",
      " 1   Date     4500 non-null   object\n",
      " 2   Like     4500 non-null   object\n",
      " 3   Content  4500 non-null   object\n",
      " 4   Hashtag  4500 non-null   object\n",
      " 5   SNS      4500 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 211.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeZ9Cz-ZZja9",
    "outputId": "50db89ba-cdf3-4b4a-95b2-8e7ee373fa6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6301 entries, 0 to 6300\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   URL      6299 non-null   object \n",
      " 1   Date     6299 non-null   object \n",
      " 2   Like     6299 non-null   float64\n",
      " 3   Content  6240 non-null   object \n",
      " 4   Hashtag  995 non-null    object \n",
      " 5   SNS      6301 non-null   object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 295.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKHM6B-AwVBe"
   },
   "source": [
    "### Content 컬럼이 missing value인 row 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtvBupqj54kg",
    "outputId": "6019bf63-26fc-46e3-ccb7-1eaa2582d374"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL          0\n",
       "Date       201\n",
       "Like       393\n",
       "Content    280\n",
       "Hashtag    670\n",
       "SNS          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmS30tgA3Z_3",
    "outputId": "d8e5515e-2064-4cb2-9ca4-5f81a0b52c01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL        0\n",
       "Date       0\n",
       "Like       0\n",
       "Content    0\n",
       "Hashtag    0\n",
       "SNS        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJ-65N5M3Z13",
    "outputId": "d70edc11-400a-4c69-e193-f7edea72b2ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL           2\n",
       "Date          2\n",
       "Like          2\n",
       "Content      61\n",
       "Hashtag    5306\n",
       "SNS           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "woiS5VPJ62GR"
   },
   "outputs": [],
   "source": [
    "def eliminate_nullrows(dataframe):\n",
    "  dataframe = dataframe.dropna(subset=['Content'])\n",
    "  dataframe.reset_index(drop=True, inplace=True)\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "MOTfZwBM3sbj"
   },
   "outputs": [],
   "source": [
    "df1 = eliminate_nullrows(df1)\n",
    "df2 = eliminate_nullrows(df2)\n",
    "df3 = eliminate_nullrows(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zvj78TBx31Zd",
    "outputId": "6ce8d376-c300-4b30-b816-569e910e7b9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL          0\n",
       "Date        93\n",
       "Like       335\n",
       "Content      0\n",
       "Hashtag    545\n",
       "SNS          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "766x6j0631Ze",
    "outputId": "5ee4fe01-d44e-4aca-ef13-010c5aa24af2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL        0\n",
       "Date       0\n",
       "Like       0\n",
       "Content    0\n",
       "Hashtag    0\n",
       "SNS        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0InYOWW131Ze",
    "outputId": "da656749-9999-462c-f827-b721e456fc59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL           0\n",
       "Date          0\n",
       "Like          0\n",
       "Content       0\n",
       "Hashtag    5245\n",
       "SNS           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qJYAXSDpfPu"
   },
   "source": [
    "### 광고성글 제외시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "ir3a0nMI0EUL"
   },
   "outputs": [],
   "source": [
    "def eliminate_ads(dataframe):\n",
    "  dataframe_treated = dataframe.query('~Content.str.contains(\"원고료|광고|홍보|협찬|협찬받아|무상|지원받아|제공받아|제공받고|당첨|이벤트|당첨 후\")', engine='python')\n",
    "  return dataframe_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ZWQmrg474J2E"
   },
   "outputs": [],
   "source": [
    "df1 = eliminate_ads(df1)\n",
    "df2 = eliminate_ads(df2)\n",
    "df3 = eliminate_ads(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdloC7Gy4Jzz",
    "outputId": "1c4c6014-917a-474b-f7e6-f9a9ba0dfd7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4286 entries, 0 to 5301\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   URL      4286 non-null   object \n",
      " 1   Date     4209 non-null   object \n",
      " 2   Like     4019 non-null   float64\n",
      " 3   Content  4286 non-null   object \n",
      " 4   Hashtag  3830 non-null   object \n",
      " 5   SNS      4286 non-null   object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gpEdWhE4Jvf",
    "outputId": "3d2b3eaf-5417-46e6-d58f-23131c4cbdc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5204 entries, 0 to 6239\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   URL      5204 non-null   object \n",
      " 1   Date     5204 non-null   object \n",
      " 2   Like     5204 non-null   float64\n",
      " 3   Content  5204 non-null   object \n",
      " 4   Hashtag  741 non-null    object \n",
      " 5   SNS      5204 non-null   object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 284.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNoLqyu3Pbsd"
   },
   "source": [
    "각 플랫폼별 8/15기준 최신 4200게시글 분석함. (null content 및 광고성 글 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "nyl3sU_4PCZK"
   },
   "outputs": [],
   "source": [
    "df_blog = df1[:4200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "DHqLnRO94Jxs"
   },
   "outputs": [],
   "source": [
    "df_insta = df2[:4200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "CpIPoERLPOzi"
   },
   "outputs": [],
   "source": [
    "df_cafe = df3[:4200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2rSCVquxIFZ"
   },
   "source": [
    "### 기본적인 문자 전처리 (띄어쓰기 적용 및 불필요한 기호 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ZiAveJBScBkq"
   },
   "outputs": [],
   "source": [
    "# 한글/영어 문자가 아닌 숫자, 기호 제외\n",
    "def apply_regular_expression(text, punc):\n",
    "    # syonlp noun extractor 사용을 위해 문장사이 doublespace 삽입\n",
    "    for ele in text:\n",
    "      if ele in punc:\n",
    "          text = text.replace(ele, \"  \")\n",
    "    pattern = re.compile('[^a-zA-Z| ㄱ-ㅣ 가-힣]')  # 추출 규칙: 영어 또는 띄어 쓰기(1 개)를 포함한 한글\n",
    "    result = pattern.sub('', text)  # 위에 설정한 \"hangul\"규칙을 \"text\"에 적용(.sub)시킴\n",
    "    #result = result.translate(str.maketrans('', '', string.punctuation)) # punctuation marks 제외\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "DEXSUu7HpOSi"
   },
   "outputs": [],
   "source": [
    "punctuation = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "df1['Content'] = df1['Content'].apply(lambda x: apply_regular_expression(x,punctuation))\n",
    "df2['Content'] = df2['Content'].apply(lambda x: apply_regular_expression(x,punctuation))\n",
    "df3['Content'] = df3['Content'].apply(lambda x: apply_regular_expression(x,punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "id": "pxk24EJESXSh",
    "outputId": "c0d83e67-d4a2-4f73-8a90-8d89ad61423b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'안녕하세요   수블린입니다   번째 용기내 챌린지는 배떡에서 로제떡볶이를 포장하기로 했어요  이전에 배떡에서 용기 가져가서 포장해온 경험이 있어요   갑자기 로제 떡볶이가 먹고 싶어서 집에서 제일 가까운 배떡에서 로제떡볶이 보통맛을 주문하기로 했어요  이미 배떡에서 용기를 가져가서 포장해 본 적이 있어서 가게로 전화하지 않고 요청사항에 개인 용기에 담아 간다고 작성을 해두었습니다   그리고 배떡 같은 경우에는 메뉴 선택하면서 단무지 필요 여부를 체크할 수 있어서 좋았어요   단무지 필요 없다고 해도 바쁘시면 단무지를 챙겨주시는 경우가 있더라고요  그래서 용기를 가지고 가기 때문에 후다닥 배떡 매장으로 갔어요  스타벅스 장바구니에 스테인리스 용기 개와 텀블러를 가지고 나갔어요   다른 것도 포장을 하기 위해 챙겨가는 센스 배떡에서 로제떡볶이 보통맛으로 주문해서 용기에 받아왔어요   제가 방문했을 때 딱 조리가 거의 끝난 상황이라서 아주 잠깐 기다렸다가 용기에 떡볶이를 받아서 나왔어요 배떡 로제떡볶이와 다른 곳에서 고기 군만두를 포장해서 같이 맛있게 잘 먹었어요  한 번에 다 먹기에는 양이 많아서 그릇에 먹을 만큼 덜어서 먹었어요   오랜만에 로제 떡볶이를 먹어서 너무너무 맛있게  잘 먹었어요 ㅠㅠ   제로웨이스트   제로웨이스트도전   용기내   용기내챌린지   다회용포장   다회용용기사용하기   떡볶이   떡복이용기내   떡볶이용기내챌린지   배떡'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[10,'Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4On0yITon8u2"
   },
   "source": [
    "#### PyKoSpacing\n",
    "\n",
    "적용했음. \n",
    "\n",
    "전희원님이 개발한 PyKoSpacing은 한국어 띄어쓰기 패키지로 띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지입니다. PyKoSpacing은 대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥 러닝 모델로 준수한 성능을 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5dyPgZsWcBmX",
    "outputId": "4c48e57f-a948-417f-ae57-e98f94ee39ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
      "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-dyzpbxr8\n",
      "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-dyzpbxr8\n",
      "  Resolved https://github.com/haven-jeon/PyKoSpacing.git to commit 7bd81fe9c55a54e1b8e6f6bd5931b87bec98784f\n",
      "Collecting tensorflow==2.5.0\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.3 MB 20 kB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from pykospacing==0.5) (3.1.0)\n",
      "Collecting argparse>=1.4.0\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.19.5)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py==3.1.0->pykospacing==0.5) (1.5.2)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.37.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.1.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.2.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.6.3)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 58.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.12.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (3.17.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.1.2)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 88.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (2.6.0)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 68.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.5.0->pykospacing==0.5) (3.3.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.34.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (57.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.4.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.3.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (4.6.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.5.0)\n",
      "Building wheels for collected packages: pykospacing\n",
      "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pykospacing: filename=pykospacing-0.5-py3-none-any.whl size=2255830 sha256=9240eb709f2b79a04d3c9d0a38f41329e0de73a430cb923ec64a1feb1061cf50\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pijs5roh/wheels/9b/93/81/a2a7dc8c66ede5bf30634d20635f32b95eac7ca2ea8844058b\n",
      "Successfully built pykospacing\n",
      "Installing collected packages: grpcio, tensorflow-estimator, keras-nightly, tensorflow, argparse, pykospacing\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.39.0\n",
      "    Uninstalling grpcio-1.39.0:\n",
      "      Successfully uninstalled grpcio-1.39.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.0\n",
      "    Uninstalling tensorflow-2.6.0:\n",
      "      Successfully uninstalled tensorflow-2.6.0\n",
      "Successfully installed argparse-1.4.0 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 pykospacing-0.5 tensorflow-2.5.0 tensorflow-estimator-2.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "argparse",
         "grpc"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDKKPbmQcBhj"
   },
   "outputs": [],
   "source": [
    "from pykospacing import Spacing\n",
    "\n",
    "def fix_spacing(text):\n",
    "  spacing = Spacing()\n",
    "  return spacing(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qoIRDNqcBeu"
   },
   "outputs": [],
   "source": [
    "dfcopy['Content'] = dfcopy['Content'].apply(lambda x: fix_spacing(x)) #헐... 35분 걸림. 다른방법을 찾아봐야겟다..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqkP1CPH1XJl"
   },
   "source": [
    "#### SOYNLP의 Normalizer활용\n",
    "\n",
    "대화 데이터, 댓글 데이터에 등장하는 반복되는 이모티콘의 정리 및 한글, 혹은 텍스트만 남기기 위한 함수를 제공합니다.\n",
    "\n",
    "예시)\n",
    "\n",
    "emoticon_normalize('ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ쿠ㅜㅜㅜㅜㅜㅜ', num_repeats=3)\n",
    "\n",
    "결과:  'ㅋㅋㅋㅜㅜㅜ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyUAMQun8QxU",
    "outputId": "20ca6720-ae24-4b42-9820-0e15e1df3cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soynlp\n",
      "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |▉                               | 10 kB 22.7 MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 20 kB 27.7 MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 30 kB 20.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 40 kB 17.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 51 kB 8.4 MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 61 kB 8.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 71 kB 8.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 81 kB 9.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 92 kB 9.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 102 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▋                       | 112 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 122 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 133 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 143 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 153 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 163 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 174 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 184 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 194 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 204 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 215 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 225 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 235 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 245 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 256 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 266 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 276 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 286 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 296 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 307 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 317 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 327 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 337 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 348 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 358 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 368 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 378 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 389 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 399 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 409 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 416 kB 7.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from soynlp) (0.22.2.post1)\n",
      "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (5.4.8)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from soynlp) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->soynlp) (1.0.1)\n",
      "Installing collected packages: soynlp\n",
      "Successfully installed soynlp-0.0.493\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "XINPHyxp1Zp-"
   },
   "outputs": [],
   "source": [
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "akal6JXg1whG"
   },
   "outputs": [],
   "source": [
    "def normalize_all(df):\n",
    "  for idx in df.index:\n",
    "    df.loc[idx,'Content'] = emoticon_normalize(df.loc[idx,'Content'], num_repeats=3) \n",
    "    df.loc[idx,'Content'] = repeat_normalize(df.loc[idx,'Content'], num_repeats=2)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mp2iX9Ry1Zl6",
    "outputId": "3cc9cf20-d352-403a-c6b8-920e4f4e66a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1763: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df_blog = normalize_all(df_blog)\n",
    "df_insta = normalize_all(df_insta)\n",
    "df_cafe = normalize_all(df_cafe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG762IAH7v4H"
   },
   "source": [
    "### 토큰화 - SOYNLP의 Noun Extractor(v.2) 활용\n",
    "\n",
    "soynlp는 품사 태깅, 단어 토큰화들을 제공함.\n",
    "비지도 학습으로 단어 토큰화 진행 - 데이터에서 자주 등장하는 단어들 (신조어 포함) 단어로 분석함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zrs1KPLOGBqQ"
   },
   "source": [
    "#### Noun Extractor ver 2\n",
    "\n",
    "soynlp=0.0.46+ 에서는 명사 추출기 version 2 를 제공합니다. 이전 버전의 명사 추출의 정확성과 합성명사 인식 능력, 출력되는 정보의 오류를 수정한 버전입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "8HYBYeh1GBPu"
   },
   "outputs": [],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.noun import LRNounExtractor_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "GdousxDlU5Pr"
   },
   "outputs": [],
   "source": [
    "postings_blog =\"\".join( df_blog['Content'].tolist())\n",
    "postings_insta =\"\".join( df_insta['Content'].tolist())\n",
    "postings_cafe =\"\".join( df_cafe['Content'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "TDZo_tOVU5Jw"
   },
   "outputs": [],
   "source": [
    "postings = [postings_blog, postings_insta, postings_cafe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAlPylsd7veW",
    "outputId": "49570873-b3df-4705-a58a-9bb999b0998a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(postings) # postings = [블로그 말뭉치, 인스타 말뭉치, 카페 말뭉치]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkgs34LE7vbW",
    "outputId": "89981b2c-f26a-41c5-8d51-bc27fe446b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 367257 from 3 sents. mem=0.639 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2105757, mem=1.394 Gb\n",
      "[Noun Extractor] batch prediction was completed for 102235 words\n",
      "[Noun Extractor] checked compounds. discovered 78476 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 99673 -> 76133\n",
      "[Noun Extractor] postprocessing ignore_features : 76133 -> 75826\n",
      "[Noun Extractor] postprocessing ignore_NJ : 75826 -> 74814\n",
      "[Noun Extractor] 74814 nouns (78476 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=1.615 Gb                    \n",
      "[Noun Extractor] 69.50 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "# min_noun_frequency default\n",
    "noun_extractor = LRNounExtractor_v2(verbose=True, extract_compound=True)\n",
    "noun_extractor.train(postings)\n",
    "nouns = noun_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PiGI-ZDvn4df",
    "outputId": "3443883d-2974-491b-9168-3f4d3cf1c727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 367257 from 3 sents. mem=1.721 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=2105757, mem=2.085 Gb\n",
      "[Noun Extractor] batch prediction was completed for 102235 words\n",
      "[Noun Extractor] checked compounds. discovered 78476 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 11694 -> 11450\n",
      "[Noun Extractor] postprocessing ignore_features : 11450 -> 11209\n",
      "[Noun Extractor] postprocessing ignore_NJ : 11209 -> 11012\n",
      "[Noun Extractor] 11012 nouns (78476 compounds) with min frequency=10\n",
      "[Noun Extractor] flushing was done. mem=2.085 Gb                    \n",
      "[Noun Extractor] 66.23 % eojeols are covered\n"
     ]
    }
   ],
   "source": [
    "# min_noun_frequency=10 지정\n",
    "nouns_minfreq = noun_extractor.train_extract(\n",
    "    postings,\n",
    "    min_noun_frequency=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtKIwOWZ7vU9",
    "outputId": "9a61c0ba-b87b-463e-a715-28ff3192eac4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NounScore(frequency=170, score=1.0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns['용기내챌린지']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3boX48klWPr8",
    "outputId": "0075581b-a96e-4c93-ac67-bbe7ebf56379"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NounScore(frequency=170, score=1.0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns_minfreq['용기내챌린지']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKp1Z3CU7vJi",
    "outputId": "b018d3eb-c0f8-41c8-a387-859733de4597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     수 (0.90)    사용 (1.00)     것 (1.00)제로웨이스트 (0.96)  플라스틱 (1.00)\n",
      "     잘 (0.40)     원 (0.97)    환경 (1.00)     위 (1.00)    너무 (0.93)\n",
      "   com (0.85)     저 (0.81)     개 (0.94)     안 (0.80)     때 (0.88)\n",
      "   친환경 (0.97)     등 (0.99)    함께 (0.91)    생각 (0.99)    실천 (1.00)\n",
      "     대 (0.97)    참여 (1.00)    제품 (1.00)     월 (0.98)    가능 (1.00)\n",
      "   그리고 (1.00)     중 (0.99)    제로 (0.96)    후기 (0.80)    정말 (0.95)\n",
      "     명 (0.99)     통 (1.00)     년 (0.99)    지구 (1.00)   있어요 (0.86)\n",
      "    우리 (0.97)     거 (0.47)    시간 (1.00)products (1.00)  웨이스트 (0.98)\n",
      "     제 (0.99)   이렇게 (0.78)   수세미 (0.97)    비누 (0.93)    위한 (0.60)\n",
      "     또 (0.60)    구매 (0.99)     물 (0.82)    내가 (0.60)   쓰레기 (0.67)\n",
      "    다양 (1.00)    요즘 (1.00)   재활용 (0.98)    조금 (1.00)     집 (0.87)\n",
      "    필요 (1.00)     곳 (1.00)     못 (0.93)    사람 (0.99)    오늘 (0.99)\n",
      "    시작 (1.00)    바로 (1.00)     맛 (0.96)     분 (0.99)    다시 (0.81)\n",
      "     책 (0.99)     후 (0.70)   그래서 (1.00)    방법 (1.00)    사진 (1.00)\n",
      "     번 (0.81)    카페 (0.92)    모두 (0.72)   때문에 (0.60)     말 (0.55)\n",
      "     시 (0.85)    지금 (0.97)   텀블러 (0.94)    칫솔 (1.00)    가장 (0.75)\n",
      "    진짜 (0.86)    이번 (1.00)    사실 (1.00)   샴푸바 (0.97)     꼭 (1.00)\n",
      "    진행 (1.00)    정도 (0.83)    비건 (0.99)    포장 (1.00)    느낌 (1.00)\n",
      "    자연 (1.00)    관심 (0.96)    직접 (0.33)    분들 (1.00)    빨대 (0.98)\n",
      "     앞 (0.76)     편 (0.98)    비닐 (1.00)    상품 (1.00)     속 (0.97)"
     ]
    }
   ],
   "source": [
    "top100 = sorted(nouns_minfreq.items(), \n",
    "    key=lambda x:-x[1].frequency)[:100]\n",
    "\n",
    "for i, (word, score) in enumerate(top100):\n",
    "    if i % 5 == 0:\n",
    "        print()\n",
    "    print('%6s (%.2f)' % (word, score.score), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-jc_fKQJtqV"
   },
   "source": [
    "### stopwords 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "osZcRQ_ZC12t",
    "outputId": "d19b647e-1187-44ab-b941-713ceff3ef7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['휴'],\n",
       " ['아이구'],\n",
       " ['아이쿠'],\n",
       " ['아이고'],\n",
       " ['어'],\n",
       " ['나'],\n",
       " ['우리'],\n",
       " ['저희'],\n",
       " ['따라'],\n",
       " ['의해']]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=[]\n",
    "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2FPKx3vHN0O"
   },
   "source": [
    "TODO: stopwords에서 '우리', '저희' 는 빼야하는것같다. 제로웨이스트의 공동체 성격을 유지하기위해서."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUtFUqPuHioa",
    "outputId": "d46ca0a7-924f-4a78-827a-92b1e6b664fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "xFr55EkuHifI"
   },
   "outputs": [],
   "source": [
    "stopwords.remove(['우리'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "5M1XF0LMIuoJ"
   },
   "outputs": [],
   "source": [
    "stopwords.remove(['저희'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "QvLgGn4JBE0V"
   },
   "outputs": [],
   "source": [
    "stopwords.remove(['함께'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9I8zyy0Kj6M",
    "outputId": "bbfb2707-4b13-4203-8ba6-b78f7148ceeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "MY3nQ-PZC16B"
   },
   "outputs": [],
   "source": [
    "zerowaste_stopwords = ['제로','웨','이스트','이스터','제로 웨이스트','제로웨이스트',\\\n",
    "                       '제로 웨이스터','제로웨이스터','웨이스트','웨이스트 ', ' 웨이스트 ',' 웨이스트',\\\n",
    "                       '환경','친환경','너무', '그리고', '정말', '이렇게', '있어요', '때문', \\\n",
    "                       '정도', '조금', '분들', '진짜', '대한', '이번', '경우', '대신', '가지고',\\\n",
    "                       '그래서','엄청','아직']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "i-oOFxCUC184"
   },
   "outputs": [],
   "source": [
    "for word in zerowaste_stopwords:\n",
    "  stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "u4svAeF5CFrH"
   },
   "outputs": [],
   "source": [
    "include = ['낮', '땅', '밭', '꽃', '돌', '멋','맛', '폼', '물', '볕', '빛', '봄', '숲', '새', '산', '숨', '싹', '옷', '잎', '차', '흙', '힘']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "pQJ2EOHUCFdn"
   },
   "outputs": [],
   "source": [
    "# 한글자 키워드 제거 (include 리스트에 있는 한글자 단어는 포함시키도록 조정)\n",
    "filtered_nouns1=[]\n",
    "\n",
    "for extract in nouns_minfreq.items():\n",
    "  x = extract[0]\n",
    "  if (len(x) > 1 or  x in include): \n",
    "    filtered_nouns1.append((x, extract[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "YID5zJ0UFK8I"
   },
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "filtered_nouns=[]\n",
    "\n",
    "for extract2 in filtered_nouns1:\n",
    "  x2 = extract2[0]\n",
    "  if x2 not in stopwords: \n",
    "    filtered_nouns.append((x2, extract2[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixx5wi8qBak_"
   },
   "source": [
    "### extract된 nouns 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "gxK5coKLMw2I"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "hy1a5o6OMwzT"
   },
   "outputs": [],
   "source": [
    "file_to_store = open(\"gdrive/My Drive/TeamProject2108/extracted_nouns_all.pkl\", \"wb\")\n",
    "pickle.dump(filtered_nouns,file_to_store)\n",
    "\n",
    "file_to_store.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_I2b7RxBwZc"
   },
   "source": [
    "### 빈도수 높은 nouns 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3vGks27ZC6xi",
    "outputId": "a2952ada-3b02-4d60-93c6-54c8ef403acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    사용 (12325.00)  플라스틱 (6842.00)   com (5069.00)    함께 (4237.00)    생각 (3948.00)\n",
      "    실천 (3897.00)    참여 (3750.00)    제품 (3616.00)    가능 (3513.00)    후기 (3261.00)\n",
      "    지구 (3117.00)    우리 (3004.00)    시간 (2969.00)products (2896.00)   수세미 (2733.00)\n",
      "    비누 (2655.00)    위한 (2637.00)    구매 (2575.00)     물 (2563.00)    내가 (2539.00)\n",
      "   쓰레기 (2523.00)    다양 (2514.00)    요즘 (2456.00)   재활용 (2377.00)    필요 (2335.00)\n",
      "    사람 (2277.00)    오늘 (2245.00)    시작 (2236.00)    바로 (2210.00)     맛 (2199.00)\n",
      "    다시 (2187.00)    방법 (2099.00)    사진 (2064.00)    카페 (2006.00)    모두 (1994.00)\n",
      "   때문에 (1978.00)    지금 (1890.00)   텀블러 (1852.00)    칫솔 (1851.00)    가장 (1829.00)\n",
      "    사실 (1785.00)   샴푸바 (1781.00)    진행 (1744.00)    비건 (1731.00)    포장 (1730.00)\n",
      "    느낌 (1728.00)    자연 (1713.00)    관심 (1697.00)    직접 (1696.00)    빨대 (1684.00)\n",
      "    비닐 (1644.00)    상품 (1586.00)   코로나 (1573.00)    물건 (1567.00)    것을 (1554.00)\n",
      "    판매 (1551.00)    건강 (1550.00)    한번 (1548.00)    이상 (1523.00)   일회용 (1516.00)\n",
      "    처음 (1481.00)    확인 (1397.00)    세제 (1360.00)    음식 (1359.00)    천연 (1315.00)\n",
      "    관련 (1306.00)    커피 (1290.00)    행주 (1288.00)    일상 (1280.00)    이용 (1268.00)\n",
      "  일회용품 (1257.00)   캠페인 (1229.00)    배출 (1226.00)    활용 (1225.00)    활동 (1217.00)\n",
      "    하루 (1207.00)    개입 (1205.00)    고민 (1194.00)    생활 (1194.00)    문제 (1185.00)\n",
      "    세트 (1179.00)    종이 (1166.00)    공간 (1161.00)    부분 (1160.00)    샴푸 (1149.00)\n",
      "    과정 (1138.00)   어떻게 (1134.00)   브랜드 (1132.00)    계속 (1131.00)    소비 (1122.00)\n",
      "    여름 (1118.00)    가격 (1110.00)    선물 (1108.00)    좋아 (1106.00)    노력 (1105.00)\n",
      "   대나무 (1096.00)   있는데 (1094.00)    성분 (1090.00)   재사용 (1086.00)    내용 (1081.00)"
     ]
    }
   ],
   "source": [
    "top100 = sorted(filtered_nouns, \n",
    "    key=lambda x:-x[1][0])[:100]\n",
    "\n",
    "for i, (word, score) in enumerate(top100):\n",
    "    if i % 5 == 0:\n",
    "        print()\n",
    "    print('%6s (%.2f)' % (word, score.frequency), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFRZYQGF7OWZ",
    "outputId": "d60222ce-66dc-4332-9072-491250d372e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    사용 (12325.00)  플라스틱 (6842.00)   com (5069.00)    함께 (4237.00)    생각 (3948.00)\n",
      "    실천 (3897.00)    참여 (3750.00)    제품 (3616.00)    가능 (3513.00)    후기 (3261.00)\n",
      "    지구 (3117.00)    우리 (3004.00)    시간 (2969.00)products (2896.00)   수세미 (2733.00)\n",
      "    비누 (2655.00)    위한 (2637.00)    구매 (2575.00)     물 (2563.00)    내가 (2539.00)\n",
      "   쓰레기 (2523.00)    다양 (2514.00)    요즘 (2456.00)   재활용 (2377.00)    필요 (2335.00)\n",
      "    사람 (2277.00)    오늘 (2245.00)    시작 (2236.00)    바로 (2210.00)     맛 (2199.00)\n",
      "    다시 (2187.00)    방법 (2099.00)    사진 (2064.00)    카페 (2006.00)    모두 (1994.00)\n",
      "   때문에 (1978.00)    지금 (1890.00)   텀블러 (1852.00)    칫솔 (1851.00)    가장 (1829.00)\n",
      "    사실 (1785.00)   샴푸바 (1781.00)    진행 (1744.00)    비건 (1731.00)    포장 (1730.00)\n",
      "    느낌 (1728.00)    자연 (1713.00)    관심 (1697.00)    직접 (1696.00)    빨대 (1684.00)\n",
      "    비닐 (1644.00)    상품 (1586.00)   코로나 (1573.00)    물건 (1567.00)    것을 (1554.00)\n",
      "    판매 (1551.00)    건강 (1550.00)    한번 (1548.00)    이상 (1523.00)   일회용 (1516.00)\n",
      "    처음 (1481.00)    확인 (1397.00)    세제 (1360.00)    음식 (1359.00)    천연 (1315.00)\n",
      "    관련 (1306.00)    커피 (1290.00)    행주 (1288.00)    일상 (1280.00)    이용 (1268.00)\n",
      "  일회용품 (1257.00)   캠페인 (1229.00)    배출 (1226.00)    활용 (1225.00)    활동 (1217.00)\n",
      "    하루 (1207.00)    개입 (1205.00)    고민 (1194.00)    생활 (1194.00)    문제 (1185.00)\n",
      "    세트 (1179.00)    종이 (1166.00)    공간 (1161.00)    부분 (1160.00)    샴푸 (1149.00)\n",
      "    과정 (1138.00)   어떻게 (1134.00)   브랜드 (1132.00)    계속 (1131.00)    소비 (1122.00)\n",
      "    여름 (1118.00)    가격 (1110.00)    선물 (1108.00)    좋아 (1106.00)    노력 (1105.00)\n",
      "   대나무 (1096.00)   있는데 (1094.00)    성분 (1090.00)   재사용 (1086.00)    내용 (1081.00)\n",
      "    주문 (1078.00)    구입 (1077.00)    감사 (1067.00)    것도 (1066.00)    운동 (1059.00)\n",
      "    기업 (1047.00)    근데 (1043.00)    선택 (1041.00)    매장 (1041.00)    쉽게 (1031.00)\n",
      "    준비 (1028.00)    피부 (1027.00)    방문 (1026.00)    추천 (1025.00)    자주 (1020.00)\n",
      "    무엇 (1019.00)    또한 (992.00)    저희 (989.00)   열심히 (981.00)   화장품 (980.00)\n",
      "    오래 (972.00)   있도록 (971.00)    소개 (967.00)   생각이 (965.00)     차 (962.00)\n",
      "    기분 (956.00)    주방 (945.00)    최근 (939.00)    뚜껑 (935.00)    걱정 (930.00)\n",
      "    물론 (928.00)   사이즈 (923.00)    엄마 (923.00)    얼마 (904.00)   설거지 (896.00)\n",
      "    매일 (891.00)    리뷰 (882.00)    보니 (882.00)    모습 (881.00)    변화 (878.00)\n",
      "  분리수거 (877.00)    생산 (871.00)    예정 (863.00)   것이다 (860.00)   아이들 (858.00)\n",
      "    등을 (854.00)  분리배출 (845.00)    더욱 (842.00)    세상 (839.00)   집에서 (833.00)\n",
      "   줄이기 (830.00)    제작 (830.00)    역시 (829.00)    여행 (823.00)    가게 (823.00)\n",
      "    음료 (819.00)    발생 (795.00)    가방 (793.00)    보관 (790.00)    중요 (783.00)\n",
      "    지역 (783.00)    의미 (781.00)    거품 (779.00)    운영 (772.00) 천연수세미 (770.00)\n",
      "    다음 (767.00)    수업 (766.00)    영상 (766.00)   실리콘 (761.00)  장바구니 (756.00)\n",
      "    치약 (755.00)   조금씩 (749.00)    cm (749.00)    개인 (746.00)  프로젝트 (737.00)\n",
      "    아침 (737.00)    신청 (736.00)    아래 (736.00)    리필 (722.00)    책을 (721.00)\n",
      "    알게 (719.00)    제거 (716.00)   온라인 (713.00)   마지막 (711.00)    따로 (707.00)\n",
      "    오일 (703.00)    소재 (703.00)    참고 (702.00)    정보 (700.00)    머리 (699.00)\n",
      "   그런데 (698.00)   있지만 (698.00)    깨끗 (697.00)    제일 (695.00)    설명 (694.00)\n",
      "    제공 (693.00)    이미 (693.00)    아이 (688.00)    자신 (688.00)    경험 (685.00)\n",
      "     옷 (681.00)    이름 (677.00)  환경보호 (675.00)제로웨이스트샵 (673.00)   디자인 (672.00)\n",
      "    안전 (671.00)    세척 (668.00)   블로그 (665.00)    친구 (665.00)    동안 (662.00)\n",
      "    사랑 (659.00)    가득 (658.00)    번째 (657.00)    시원 (656.00)    서울 (656.00)\n",
      "    알고 (654.00)    자리 (652.00)   생분해 (651.00)    공유 (650.00)    박스 (650.00)\n",
      "   마스크 (648.00)    도전 (646.00)    그럼 (643.00)  재활용이 (641.00)    이후 (641.00)\n",
      "   그렇게 (640.00) 업사이클링 (637.00)    과일 (637.00)    나무 (637.00)    링크 (632.00)\n",
      "    결국 (631.00)   포스팅 (630.00)    사회 (630.00)    깔끔 (629.00)    세계 (626.00)\n",
      "   거품이 (624.00)    배달 (624.00)    재료 (624.00)   손수건 (619.00)   에코백 (618.00)\n",
      "    부탁 (613.00)    정리 (608.00)   그대로 (606.00)    문의 (606.00)    미션 (606.00)\n",
      "    수도 (606.00)   서비스 (605.00)    건조 (602.00)    그릇 (600.00)    종류 (600.00)\n",
      "   도움이 (599.00)   사람들 (599.00)    간단 (599.00)   줄이고 (598.00)  프로그램 (597.00)\n",
      "    자체 (595.00)   폐기물 (593.00)   유기농 (591.00)    평소 (590.00)    ㅠㅠ (588.00)\n",
      "    영향 (586.00)    계획 (585.00)    행복 (583.00)   사장님 (576.00)    지속 (575.00)\n",
      "    수거 (575.00)    오후 (574.00)    상황 (572.00)    재질 (572.00)    기억 (566.00)\n",
      "    인간 (565.00)  기후변화 (563.00)    기존 (563.00)    저녁 (562.00)    행동 (561.00)\n",
      "    kr (561.00)    고체 (560.00)    모양 (559.00)    주변 (557.00)   라이프 (554.00)\n",
      "   제대로 (554.00)   하나씩 (553.00)    완전 (553.00)   페트병 (552.00)    살짝 (551.00)\n",
      "    실제 (549.00)  고체치약 (544.00)  버려지는 (544.00)   최대한 (544.00)    채식 (543.00)\n",
      "   챌린지 (539.00)    발견 (538.00)    추가 (537.00)   이야기 (536.00)    상태 (536.00)\n",
      "    택배 (535.00)     새 (534.00)    이거 (529.00)    등등 (529.00)    전혀 (521.00)\n",
      "    해결 (519.00)    보호 (519.00)    마음 (518.00)    예전 (518.00)    가치 (517.00)\n",
      "    이것 (517.00)    기대 (511.00)    만원 (510.00)   여러분 (508.00)    인증 (505.00)\n",
      "  지속가능 (504.00)   것으로 (504.00)    만족 (504.00)    원래 (504.00)    궁금 (502.00)\n",
      "    구성 (500.00)   포장재 (499.00)    두피 (497.00)    약간 (497.00)    메뉴 (494.00)\n",
      "   대부분 (493.00)    한국 (493.00)  알맹상점 (490.00)  있답니다 (490.00)    것들 (489.00)\n",
      "   물티슈 (488.00)    관리 (485.00)   음식물 (482.00)    효과 (482.00)    소창 (481.00)\n",
      "    저렴 (478.00)   동구밭 (477.00)    자원 (476.00)    도착 (473.00)    근처 (472.00)\n",
      "    기준 (472.00)  친환경적 (471.00)    기회 (468.00)    점점 (468.00)    혼자 (467.00)\n",
      "미세플라스틱 (466.00)   다회용 (466.00)    않기 (466.00)    습관 (466.00)    ml (466.00)\n",
      "    식물 (465.00)    대체 (463.00)   만들고 (461.00)    안심 (461.00)    단계 (460.00)\n",
      "    오픈 (459.00)    올해 (459.00)   클래스 (457.00)    개발 (456.00)    동참 (455.00)\n",
      "   밀랍랩 (453.00)    자세 (453.00)   원하는 (450.00)   주세요 (450.00)    동네 (450.00)\n",
      "    요리 (450.00)   같아서 (448.00)    미래 (445.00)    대표 (445.00)   소프넛 (444.00)\n",
      "    할인 (444.00)    주말 (443.00)    회사 (443.00)    미리 (443.00)    투명 (442.00)\n",
      "   있다면 (440.00)    시대 (439.00)     꽃 (437.00)    작성 (436.00)    오염 (435.00)\n",
      "    불편 (432.00)    사업 (432.00)    수건 (430.00)    가족 (430.00)    처리 (429.00)\n",
      "    방식 (429.00)    봉투 (429.00)    혹시 (429.00) 인스타그램 (428.00)    기본 (427.00)\n",
      "   ESG (426.00)    기부 (425.00)    탄소 (425.00)    검색 (424.00)    완성 (424.00)\n",
      "    갖고 (424.00)    키트 (424.00)    개월 (424.00)    보통 (423.00)    그것 (421.00)\n",
      "   제품들 (419.00)  주방세제 (418.00)    물품 (418.00)    증가 (417.00)    위치 (416.00)\n",
      "    댓글 (415.00)  환경오염 (413.00)   냉장고 (413.00)    포함 (413.00)    어제 (411.00)\n",
      "    장점 (411.00)   불필요 (410.00)    나를 (410.00)    부담 (409.00)    향이 (409.00)\n",
      "  있었어요 (408.00) 대나무칫솔 (407.00)    지원 (407.00)   마무리 (406.00)    얼굴 (406.00)\n",
      "   최소화 (405.00)    남편 (405.00)   플로깅 (404.00)   덕분에 (403.00)  있었는데 (402.00)\n",
      "    살림 (402.00)    주차 (401.00)    국내 (400.00)    배송 (400.00)  설거지바 (398.00)\n",
      "   꾸준히 (397.00)    필수 (396.00)    부족 (395.00)    형태 (395.00)   개인적 (394.00)\n",
      "   에너지 (393.00)    나름 (393.00)    동물 (393.00)   사회적 (392.00)    채소 (392.00)\n",
      "    사서 (392.00)    체험 (391.00)    착한 (390.00)   인스타 (389.00)    기술 (389.00)\n",
      "    교육 (388.00)    삼베 (387.00)   디저트 (386.00)   네이버 (383.00)  같습니다 (382.00)\n",
      "   않을까 (382.00)   레시피 (381.00)    위에 (381.00)   않아도 (380.00)    제주 (380.00)\n",
      "    장소 (380.00)    사이 (379.00)    사고 (379.00)    유지 (378.00)    결과 (378.00)\n",
      "   도시락 (376.00)    분리 (375.00)    예약 (375.00)   편하게 (374.00)    완벽 (374.00)\n",
      "    원단 (374.00)  주방비누 (373.00)    기록 (373.00)    이러 (373.00)  환경문제 (372.00)\n",
      "   식재료 (372.00)   싶어서 (372.00)    이곳 (371.00)  소창행주 (369.00)   유튜브 (369.00)\n",
      "    지향 (367.00)    늘어 (366.00)   좋아서 (365.00)    순간 (365.00)    뿌듯 (365.00)\n",
      "   화이트 (364.00)   스스로 (363.00)    심각 (363.00)    편리 (363.00)    않게 (363.00)\n",
      "    기후 (363.00)   그동안 (362.00)    세탁 (362.00)    우선 (360.00)    시장 (359.00)\n",
      "   케이크 (358.00)    그림 (358.00)    목표 (358.00)    어느 (358.00)   배출량 (355.00)\n",
      "   너무나 (354.00)    기간 (354.00)    일을 (353.00)   샐러드 (352.00)    작업 (352.00)\n",
      "  우리나라 (351.00)   일주일 (351.00)    미국 (351.00)    하늘 (351.00)    내일 (350.00)"
     ]
    }
   ],
   "source": [
    "top500 = sorted(filtered_nouns, \n",
    "    key=lambda x:-x[1][0])[:500]\n",
    "\n",
    "for i, (word, score) in enumerate(top500):\n",
    "    if i % 5 == 0:\n",
    "        print()\n",
    "    print('%6s (%.2f)' % (word, score.frequency), end='')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SNS_txtdata_preprocessing_and_topicModeling2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
