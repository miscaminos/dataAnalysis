{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d89d9f",
   "metadata": {},
   "source": [
    "### Naver 블로그 \n",
    "\n",
    "블로그 텍스트 데이터 웹 크롤링 using selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf43b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 setup\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url 추출하기 method정의\n",
    "\n",
    "def get_urls(query, totalCount, startDate, endDate):\n",
    "    url_list_recent = [] \n",
    "    \n",
    "    if totalCount%7:\n",
    "        lastPage = int(totalCount/7) + 1\n",
    "    else:\n",
    "        lastPage = int(totalCount/7)\n",
    "\n",
    "    for i in range(1,lastPage+1):\n",
    "        #url='https://section.blog.naver.com/Search/Post.naver?pageNo='+ str(i) + '&rangeType=PERIOD&orderBy=sim&startDate=2020-08-15&endDate=2021-08-15&keyword='+query\n",
    "        url='https://section.blog.naver.com/Search/Post.naver?pageNo='+ str(i) + '&rangeType=PERIOD&orderBy=recentdate&startDate=2021-08-01&endDate=2021-08-15&keyword='+query\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        for j in range(1, 10): # 각 블로그 주소 저장\n",
    "            try: \n",
    "                titles = driver.find_element_by_xpath('/html/body/ui-view/div/main/div/div/section/div[2]/div['+str(j)+']/div/div[1]/div[1]/a[1]')\n",
    "                href = titles.get_attribute('href')\n",
    "                url_list_recent.append(href)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    url_list_recent = list(set(url_list_recent))\n",
    "    return url_list_recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a482fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_url 실행 및 csv로 저장하기\n",
    "\n",
    "# 크롬 웹브라우저 실행\n",
    "path = 'C:/SJL/GyeonggidoData_Course/Teamproject/Chrome/chromedriver.exe'\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "query = '\"제로 웨이스트\"'\n",
    "startDate='2021-07-15'\n",
    "endDate='2021-08-15'\n",
    "totalCount=2500\n",
    "\n",
    "url_list = get_urls(query, totalCount, startDate, endDate)\n",
    "driver.close()\n",
    "\n",
    "x = pd.Series(url_list)\n",
    "x.to_csv('./zerowaste/naverblog_urls.csv', index=False)\n",
    "\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list=[] \n",
    "like_list=[]\n",
    "content_list = []\n",
    "hashtag_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a65faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posting 내용 추출하기 method정의\n",
    "\n",
    "def get_postings(u_list):\n",
    "    url_list = list(u_list['URL']) \n",
    "    count=0\n",
    "\n",
    "    for link in url_list: # 수집한 url 만큼 반복\n",
    "        driver.get(link) # 해당 url로 이동\n",
    "        try:\n",
    "            driver.switch_to.frame('mainFrame')\n",
    "            time.sleep(1.5)\n",
    "\n",
    "            # date\n",
    "            publish_date='.blog2_container'\n",
    "            dates = driver.find_elements_by_css_selector(publish_date)\n",
    "            datetime=\"\"\n",
    "            for d in dates: datetime+=d.text\n",
    "            date_list.append(datetime)\n",
    "\n",
    "            # like\n",
    "            u_like='.wrap_postcomment'\n",
    "            likes = driver.find_elements_by_css_selector(u_like)\n",
    "            like_counts=\"\"\n",
    "            for k in likes[1:2]: like_counts+=k.text\n",
    "            like_list.append(like_counts)\n",
    "\n",
    "            # content\n",
    "            overlays = \".se-component.se-text.se-l-default\"\n",
    "            contents = driver.find_elements_by_css_selector(overlays)\n",
    "            posting_contents=\"\"\n",
    "            for content in contents:\n",
    "                posting_contents+=content.text\n",
    "            content_list.append(posting_contents)\n",
    "\n",
    "            # hashtag\n",
    "            try:\n",
    "                htags='.wrap_tag'\n",
    "                tags = driver.find_elements_by_css_selector(htags)\n",
    "                hash_tags=\"\"\n",
    "                for h in tags: hash_tags+=h.text\n",
    "            except:\n",
    "                pass\n",
    "            hashtag_list.append(hash_tags)\n",
    "\n",
    "            count+=1\n",
    "            if count%100==0:\n",
    "                print(count, 'postings scraped')\n",
    "        except:\n",
    "            pass\n",
    "    print(\"url counts: \",len(url_list))\n",
    "    print(\"date counts: \",len(date_list))\n",
    "    print(\"like counts: \", len(like_list))\n",
    "    print(\"content counts: \",len(content_list))\n",
    "    print(\"hashtag: \", len(hashtag_list))\n",
    "\n",
    "    all_data = pd.DataFrame(data={'URL':url_list, 'Date':date_list, 'Like': like_list, \n",
    "            'Content':content_list, 'Hashtag':hashtag_list})    \n",
    "        \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce620cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추출 dataframe내 불필요 문자/기호 전처리하는 method 정의\n",
    "\n",
    "def preprocess_data(dfcopy):\n",
    "    \n",
    "    for idx in dfcopy.index:\n",
    "        dfcopy.loc[idx,'Date'] = re.sub('[a-zA-Zㄱ-힗]','',str(dfcopy.loc[idx,'Date']))\n",
    "        try:\n",
    "            dfcopy.loc[idx,'Date'] = str(dfcopy.loc[idx,'Date']).split(\"・\")[1]\n",
    "        except:\n",
    "            dfcopy.loc[idx,'Date'] = \"\"\n",
    "\n",
    "        dfcopy.loc[idx,'Like'] = str(dfcopy.loc[idx,'Like']).split(\"\\n\")[0]\n",
    "        dfcopy.loc[idx,'Like'] = re.findall(\"\\d+\",str(dfcopy.loc[idx,'Like']))\n",
    "        try:\n",
    "            dfcopy.loc[idx,'Like'] = dfcopy.loc[idx,'Like'][0]\n",
    "        except:\n",
    "            dfcopy.loc[idx,'Like'] = \"\"\n",
    "\n",
    "        dfcopy.loc[idx,'Content'] = str(dfcopy.loc[idx,'Content']).replace('\\n',' ')\n",
    "        dfcopy.loc[idx,'Hashtag'] = str(dfcopy.loc[idx,'Hashtag']).replace('\\n',' ')\n",
    "        \n",
    "    return dfcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea36a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url list 불러오기 및 posting 내용 가져오기 실행\n",
    "\n",
    "u_list = pd.read_csv('./zerowaste/naverblog_urls.csv')\n",
    "\n",
    "# 크롬 웹브라우저 실행\n",
    "path = 'C:/SJL/GyeonggidoData_Course/Teamproject/Chrome/chromedriver.exe'\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "data = get_postings(u_list)\n",
    "driver.close()\n",
    "\n",
    "data_treated = preprocess_data(data)\n",
    "data_treated.head()\n",
    "\n",
    "data_treated.to_csv('./zerowaste/naverblogs_crawling_selenium2.csv',index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataframe 구성 확인\n",
    "\n",
    "data_treated.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniprojects",
   "language": "python",
   "name": "miniprojects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
