## requests lib 사용 및 header위장하기

### 1. header위장
python으로 접속하면 data crawling 목적이라는 것을 naver(또는 crawling하려는 website)의 서버에서 인식하고 데이터를 주지않는다.

그래서 Chrome browser에서 접속하는것 처럼 header를 위장한다.

### 2. iframe안의 데이터를 가져와야할때 
https://finance.naver.com/item/sise.nhn?code=005930 이 링크에 들어가면 페이지 맨아래에 보이는 '일별시세' 데이터를 가져오려하는데, 해당 데이터는 조회한 페이지내 iframe 태그에 들어가있다.

iframe 태그로 구분되어있는 데이터는 해당 태그의 src로 다시 들어가야한다. 
거기서 페이지 소스코드로부터 데이터를 추출해야한다. 이번 실습에서는 

    https://finance.naver.com/item/sise_day.nhn?code=005930

링크로 들어가야지만, 일별시세 iframe내의 데이터를 추출해올수 있다.

### 3. requests의 get() 함수 & pandas의 read_html()함수
get(url)함수로 해당 url요청이 server에 보내졌을때, server로부터 response를 받아올 수 있다.
받은 response의 text만 추출하면 response content를 한번에 조회할 수 있다.

text내용이 table형태이라면, pandas의 read_html()함수를 통해 바로
dataframe으로 받아올 수 있다.

e.g., price = pd.read_html(r.text)[0] #price라는 dataframe에 저장

참고 link: https://docs.python-requests.org/en/master/user/quickstart/#make-a-request


```python
import requests
import pandas as pd
```


```python
url = "https://finance.naver.com/item/sise_day.nhn?code=005930"
```


```python
#python이 아닌 chorme으로 접속하는것처럼 보이게 header에 user-agent를 아래와 같이 지정한다
header = {'user-agent':
          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36'}
```


```python
# requests.get 함수를 통해 해당 url내 소스코드를 가져온다
r = requests.get(url, headers=header)
```


```python
r.text
type(r.text) #문자열로 데이터를 추출해올수있다. 
```




    str



#### 요청 url에대한 response내용을 pandas dataframe으로 저장하기


```python
#요청url내 page번호를 forloop으로 바꾸어가며 각 url요청에 대한 rseponse를 받아서 list로 저장한다.

price_list=[]
url = "https://finance.naver.com/item/sise_day.nhn?code=005930&page={}"

for j in range(1,11):
    url = url.format(j)
    r = requests.get(url, headers = header)
    price = pd.read_html(r.text)[0].dropna() #html에 table형태의 데이터를 바로 dataframe으로 저장
    price_list.append(price)
    
stock = pd.concat(price_list)
```


```python
stock.info()
```

    <class 'pandas.core.frame.DataFrame'>
    Int64Index: 100 entries, 1 to 13
    Data columns (total 7 columns):
     #   Column  Non-Null Count  Dtype  
    ---  ------  --------------  -----  
     0   날짜      100 non-null    object 
     1   종가      100 non-null    float64
     2   전일비     100 non-null    float64
     3   시가      100 non-null    float64
     4   고가      100 non-null    float64
     5   저가      100 non-null    float64
     6   거래량     100 non-null    float64
    dtypes: float64(6), object(1)
    memory usage: 6.2+ KB
    

## 실습: 종목별 시세정보 수집

### KOSPI 종목 4월 한달치 수집 -> 하나의 DataFrame으로 저장하기

krx 정보데이터시스템에서 kospi 종목을 가져와서 네이버 금융해서 각 종목의 4월 한달치 시세 데이터를 수집하기
(그리고 수집한 데이터를 dataframe으로 저장하기)

추출하려는 데이터가 server로 부터 어떻게 전송되어 오는지는 (F12)개발자 도구의 **Network** tab에서 확인할 수 있다. 

네이버 금융에서 특정 종목의 '차트'페이지의 경우 ajax방식으로 교환되는 데이터의 상세내용을 조회할수있다.

Name = "https://api.finance.naver.com/siseJson.naver?symbol=005930&requestType=2&count=1&startTime=20210503&timeframe=day"에 해당하는 상세정보를 조회하면,

General, Response Headers, Request Headers, Query String Parameters를 확인할 수 있다.

Requests Headers의 method는 POST로 지정되어있다. 여기에서 **Query String Parameter**들이 요청에 들어가는 parameter들이다. 

post방식 요청의 parameter값들을 수정해서 내가 원하는 데이터를 server로부터 받아올 수 있다.


```python
#네이버 금융에서 종목별 시세 데이터 가져오기

post_url = "https://api.finance.naver.com/siseJson.naver?"

# Query String Parameter 지정
payload = {'symbol': '097870', #종목을 구분하는 symbol
           'requestType': '1',
            'startTime': '20171013', 
            'endTime': '20180628', 
           'timeframe': 'week'}

r = requests.post(post_url, data=payload, headers=header)
```


```python
r.status_code #200=성공
```




    200




```python
#종목코드 097870에 해당하는 기업의 해당 기간내 주별 시세 데이터를 가져온다.
r.text
```

### krx 데이터 시스템에서 종목코드 가져오기


```python
krx_post = "http://data.krx.co.kr/comm/bldAttendant/getJsonData.cmd"

krx_payload = {'bld': 'dbms/MDC/STAT/standard/MDCSTAT01501',
                'mktId': 'STK',
                'trdDd': '20210401',
                'share': '1',
                'money': '1',
                'csvxls_isNo': 'false'}
```


```python
krx_data = requests.post(krx_post, data = krx_payload, headers=header)
```

### post방식으로 넘어오는 json형태의 데이터 다루기


```python
import json
```


```python
text = json.loads(krx_data.text)
```


```python
text #json형태의 데이터 출력
```


```python
krx_df = pd.DataFrame(krx_data.json()['OutBlock_1'])
krx_df.head(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ISU_SRT_CD</th>
      <th>ISU_ABBRV</th>
      <th>MKT_NM</th>
      <th>SECT_TP_NM</th>
      <th>TDD_CLSPRC</th>
      <th>FLUC_TP_CD</th>
      <th>CMPPREVDD_PRC</th>
      <th>FLUC_RT</th>
      <th>TDD_OPNPRC</th>
      <th>TDD_HGPRC</th>
      <th>TDD_LWPRC</th>
      <th>ACC_TRDVOL</th>
      <th>ACC_TRDVAL</th>
      <th>MKTCAP</th>
      <th>LIST_SHRS</th>
      <th>MKT_ID</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>095570</td>
      <td>AJ네트웍스</td>
      <td>KOSPI</td>
      <td></td>
      <td>4,620</td>
      <td>1</td>
      <td>115</td>
      <td>2.55</td>
      <td>4,540</td>
      <td>4,625</td>
      <td>4,480</td>
      <td>162,695</td>
      <td>742,284,000</td>
      <td>216,319,002,900</td>
      <td>46,822,295</td>
      <td>STK</td>
    </tr>
    <tr>
      <th>1</th>
      <td>006840</td>
      <td>AK홀딩스</td>
      <td>KOSPI</td>
      <td></td>
      <td>26,900</td>
      <td>1</td>
      <td>200</td>
      <td>0.75</td>
      <td>26,800</td>
      <td>27,450</td>
      <td>26,750</td>
      <td>34,663</td>
      <td>936,083,050</td>
      <td>356,359,390,900</td>
      <td>13,247,561</td>
      <td>STK</td>
    </tr>
    <tr>
      <th>2</th>
      <td>027410</td>
      <td>BGF</td>
      <td>KOSPI</td>
      <td></td>
      <td>7,270</td>
      <td>1</td>
      <td>260</td>
      <td>3.71</td>
      <td>7,140</td>
      <td>7,500</td>
      <td>7,110</td>
      <td>3,360,279</td>
      <td>24,598,878,940</td>
      <td>695,861,070,570</td>
      <td>95,716,791</td>
      <td>STK</td>
    </tr>
  </tbody>
</table>
</div>




```python
krx_df['mkt_nm'.upper()] == 'kospi'.upper() 
#내가 krx사이트에서 KOSPI만 가져왔기때문에 923개 모두 True
#만약 다른 markt name과 섞여 추출했다면 여기서 filter하면된다.
```


```python
#종목코드 배열
krx_df[krx_df['mkt_nm'.upper()]=='kospi'.upper()]['ISU_SRT_CD'].values
```

### 시세데이터 추출
그 다음, 네이버 금융에서 종목별 시세 데이터를 문자열에 어떻게 추출할지 보자.


```python
# 종목하나의 시세가 어떻게 추출되지?
post_url = "https://api.finance.naver.com/siseJson.naver?"
payload = {'symbol': '097870', #종목을 구분하는 symbol
           'requestType': '1',
            'startTime': '20171013', 
            'endTime': '20180628', 
           'timeframe': 'week'}
r = requests.post(post_url, data=payload, headers=header)
r.text
```


```python
#방법1

url = "https://finance.naver.com/item/sise_day.nhn?code={}&page=1"
kospi_total = []
for i, code in enumerate(krx_df[krx_df['mkt_nm'.upper()]=='kospi'.upper()]['ISU_SRT_CD'].values):
    if i%100 == 0 and i>0:
        break
    r = requests.get(url.format(code), data=payload, headers=header)
    tmp_df = pd.read_html(r.text)[0].dropna()
    tmp_df['code'] = code #종목코드 열 생성
    kospi_total.append(tmp_df)

```


```python
result = pd.concat(kospi_total)
result
```


```python
#방법2

kospi_total = []
for idx, code in enumerate(krx_df[krx_df['mkt_nm'.upper()] == 'kospi'.upper()]['ISU_SRT_CD'].values):
    if idx % 100 == 0 and idx > 0 :
        break
    payload['symbol'] = code
    r = requests.post(post_url, data=payload, headers=header)
    break
    tmp_df = pd.read_html(r.text)[0].dropna()
    tmp_df['code'] = code
    kospi_total.append(tmp_df)
```


```python
kospi_total
```


```python
r.text.split('\n\t\t\n')[1].split(",")
```

### 정규식 Regular Expression으로 추출하 문자열 데이터를 처리할수도 있다.

특히, NLP 관련 작업에서 regular expression이 유용하게 사용된다.

regular expression 사용에 익숙해지려면 많은 연습이 필요함.


```python
import re
```


```python
p = "((?<=\[)[^)]*(?=\]))"
```


```python
pattern = re.compile(p)
```


```python
r.text.split("\n")[6]
```




    '["20171013", 6887, 7058, 6784, 6887, 92646, 1.0],'




```python
pattern.findall(r.text.split("\n")[6])
```




    ['"20171013", 6887, 7058, 6784, 6887, 92646, 1.0']




```python
rt = [x.replace('"', "").strip() for x in pattern.findall(r.text.split("\n")[6])[0].split(",")]
```


```python
rt
```




    ['20171013', '6887', '7058', '6784', '6887', '92646', '1.0']




```python
total=[]
for x in r.text.split("\n"):
    rt = pattern.findall(x)
    if len(rt) >= 1:
        total.append(rt[0].split(","))
```
