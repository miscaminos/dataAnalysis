{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb8b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03436378",
   "metadata": {},
   "source": [
    "#### 주제 검색어 검색결과에서 블로그 글 url 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea64411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롬 웹브라우저 실행\n",
    "path = 'C:/SJL/GyeonggidoData_Course/Teamproject/Chrome/chromedriver.exe'\n",
    "driver = webdriver.Chrome(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url 추출하기 (검색어, 기간 설정 날짜, 전체 수 하드코딩됨. 수정필요)\n",
    "\n",
    "url_list_recent = [] \n",
    "query = \"친환경 세제\"\n",
    "totalCount=78670\n",
    "\n",
    "if totalCount%8:\n",
    "    lastPage = int(totalCount/8) + 1\n",
    "else:\n",
    "    lastPage = int(totalCount/8)\n",
    "\n",
    "for i in range(1,lastPage+1):\n",
    "    url='https://section.blog.naver.com/Search/Post.naver?pageNo='+ str(i) + '&rangeType=PERIOD&orderBy=recentdate&startDate=2020-08-15&endDate=2021-08-15&keyword='+query\n",
    "    driver.get(url)\n",
    "    time.sleep(0.5)\n",
    " \n",
    "    for j in range(1, 8): # 각 블로그 주소 저장\n",
    "        try: \n",
    "            titles = driver.find_element_by_xpath('/html/body/ui-view/div/main/div/div/section/div[2]/div['+str(j)+']/div/div[1]/div[1]/a[1]')\n",
    "            href = titles.get_attribute('href')\n",
    "            url_list_recent.append(href)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6733c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(url_list_recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(url_list_recent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76586863",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(url_list_recent)\n",
    "x.to_csv('naverblog_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c3322e",
   "metadata": {},
   "source": [
    "#### 수집한 url을 순차적으로 조회해서 본문 scraping하기 using requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a0acfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iframe 제거 후 blog.naver.com 붙이기\n",
    "def delete_iframe(url):\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status() # 문제시 프로그램 종료\n",
    "    soup = BeautifulSoup(res.text, \"lxml\") \n",
    "\n",
    "    src_url = \"https://blog.naver.com/\" + soup.iframe[\"src\"]\n",
    "    \n",
    "    return src_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f4c94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 스크래핑\n",
    "def text_scraping(url):\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status() # 문제시 프로그램 종료\n",
    "    soup = BeautifulSoup(res.text, \"lxml\") \n",
    "\n",
    "    if soup.find(\"div\", attrs={\"class\":\"se-main-container\"}):\n",
    "        text = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).get_text()\n",
    "        text = text.replace(\"\\n\",\"\") #공백 제거\n",
    "        print(\"블로그\")\n",
    "        return text\n",
    "    else:\n",
    "        return \"확인불가\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9cd8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 + date, like, hashtag 스크래핑\n",
    "def all_scraping(url):\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status() # 문제시 프로그램 종료\n",
    "    soup = BeautifulSoup(res.text, \"lxml\") \n",
    "\n",
    "    # date\n",
    "    try:\n",
    "        datetime = soup.find_all(\"span\",{\"class\":'se_publishDate pcol2'})\n",
    "    except:\n",
    "        datetime=\"None\"\n",
    "    \n",
    "    # like\n",
    "    try:\n",
    "        likes = soup.find_all('div',{'class':'u_likeit_list_module _reactionModule'})\n",
    "        like_counts=soup.select_one(\"div em\")\n",
    "    except:\n",
    "        like_counts=\"None\"\n",
    "    \n",
    "    # 본문 content\n",
    "    try:\n",
    "        text = soup.find(\"div\", attrs={\"class\":\"se-main-container\"}).get_text()\n",
    "        text = text.replace(\"\\n\",\"\") #공백 제거\n",
    "    except:\n",
    "        text = \"None\"\n",
    "    \n",
    "    # hashtag\n",
    "    try:\n",
    "        htags = soup.find_all('div',{'class':'wrap_tag'})\n",
    "        hash_tags=\"\"\n",
    "        for h in htags:\n",
    "            hash_tags+=h.text\n",
    "    except:\n",
    "        hash_tags=\"None\"\n",
    "    \n",
    "    od = OrderedDict()\n",
    "    od['URL'] = url\n",
    "    od['Date']= datetime\n",
    "    od['Like']=like_counts\n",
    "    od['Content']= text\n",
    "    od['Hashtag']=hash_tags\n",
    "    \n",
    "    return od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "090a3ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = pd.read_csv('naverblog_urls.csv')\n",
    "url_list=urls['0']\n",
    "url_list = list(set(url_list))\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "cnt=1\n",
    "for r in url_list:\n",
    "    try: \n",
    "        url = delete_iframe(r)\n",
    "        posting = all_scraping(url)\n",
    "        data.append(posting)\n",
    "        cnt+=1\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fdb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('naverblogs_crawling_requests2.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9b7e44",
   "metadata": {},
   "source": [
    "#### 수집한 url을 순차적으로 조회해서 본문 scraping하기 using selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75371c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "\n",
    "date_list=[] \n",
    "like_list=[]\n",
    "content_list = []\n",
    "hashtag_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url list 불러오기\n",
    "import pandas as pd\n",
    "\n",
    "u_list = pd.read_csv('./selenium_2months/naverblog_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd87696",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = list(u_list['URL']) #2개월 --> 7,929개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "\n",
    "# 크롬 웹브라우저 실행\n",
    "path = 'C:/SJL/GyeonggidoData_Course/Teamproject/Chrome/chromedriver.exe'\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "for link in url_list: # 수집한 url 만큼 반복\n",
    "    driver.get(link) # 해당 url로 이동\n",
    "    try:\n",
    "        driver.switch_to.frame('mainFrame')\n",
    "        time.sleep(0.7)\n",
    "\n",
    "        # date\n",
    "        publish_date='.blog2_container'\n",
    "        dates = driver.find_elements_by_css_selector(publish_date)\n",
    "        datetime=\"\"\n",
    "        for d in dates: datetime+=d.text\n",
    "        date_list.append(datetime)\n",
    "\n",
    "        # like\n",
    "        u_like='.wrap_postcomment'\n",
    "        likes = driver.find_elements_by_css_selector(u_like)\n",
    "        like_counts=\"\"\n",
    "        for k in likes[1:2]: like_counts+=k.text\n",
    "        like_list.append(like_counts)\n",
    "\n",
    "        # content\n",
    "        overlays = \".se-component.se-text.se-l-default\"\n",
    "        contents = driver.find_elements_by_css_selector(overlays)\n",
    "        posting_contents=\"\"\n",
    "        for content in contents:\n",
    "            posting_contents+=content.text\n",
    "        content_list.append(posting_contents)\n",
    "\n",
    "        # hashtag\n",
    "        try:\n",
    "            htags='.wrap_tag'\n",
    "            tags = driver.find_elements_by_css_selector(htags)\n",
    "            hash_tags=\"\"\n",
    "            for h in tags: hash_tags+=h.text\n",
    "        except:\n",
    "            pass\n",
    "        hashtag_list.append(hash_tags)\n",
    "        \n",
    "        count+=1\n",
    "        if count%100==0:\n",
    "            print(count, 'postings scraped')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make into dataframe\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataframe = pd.DataFrame(\n",
    "    data={\n",
    "        'URL':url_list, 'Date':date_list, 'Like': like_list, \n",
    "        'Content':content_list, 'Hashtag':hashtag_list\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d119725",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('naverblogs_crawling_selenium.csv',index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniprojects",
   "language": "python",
   "name": "miniprojects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
